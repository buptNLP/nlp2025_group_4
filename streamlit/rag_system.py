import streamlit as st
import chromadb
from chromadb.utils import embedding_functions
import requests
import json
import re
import csv
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import asdict

from config import HAS_RERANKER
from models import AncientTextSegment
from text_processing import SmartTextChunker, AncientTextAnalyzer
from retrieval import HybridRetriever

from config import HAS_BM25, HAS_RERANKER
from api_client import get_api_client, APIProvider  # ä½¿ç”¨æ–°çš„ API å®¢æˆ·ç«¯å·¥å‚

if HAS_RERANKER:
    import torch

class UniversalAncientRAG:
    """é€šç”¨å¤æ–‡RAGç³»ç»Ÿ"""
    
    def __init__(self, embedding_model: str = "BAAI/bge-large-zh-v1.5",
             max_chunk_size: int = 150,
             min_chunk_size: int = 20,
             context_window: int = 80):
        self.client = chromadb.Client()
        self.collection_name = "ancient_texts_collection"
        self.embedding_model = embedding_model
        self.segments: List[AncientTextSegment] = []
        
        # ä½¿ç”¨é…ç½®å‚æ•°åˆå§‹åŒ–åˆ†å—å™¨
        self.chunker = SmartTextChunker(
            max_chunk_size=max_chunk_size,
            min_chunk_size=min_chunk_size,
            context_window=context_window
        )
        
        self.analyzer = AncientTextAnalyzer()
        self.retriever: Optional[HybridRetriever] = None
        self.api_client = None
            
        # é…ç½®embeddingå‡½æ•°
        self._setup_embedding_function()
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._initialize_collection()

    def set_api_config(self, config: Dict[str, Any]):
        """è®¾ç½®APIé…ç½®å¹¶åˆ›å»ºç›¸åº”çš„å®¢æˆ·ç«¯"""
        try:
            # ä½¿ç”¨å·¥å‚æ–¹æ³•åˆ›å»º API å®¢æˆ·ç«¯
            self.api_client = get_api_client(config)
    
            # å­˜å‚¨å½“å‰é…ç½®ï¼ˆç”¨äºæ˜¾ç¤ºçŠ¶æ€ï¼‰
            self.current_api_config = config
            
        except Exception as e:
            st.error(f"è®¾ç½® API å®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            self.api_client = None

    def update_chunker_params(self, max_chunk_size: int = None, 
                         min_chunk_size: int = None, 
                         context_window: int = None):
        """æ›´æ–°åˆ†å—å™¨å‚æ•°"""
        if max_chunk_size is not None:
            self.chunker.max_chunk_size = max_chunk_size
        if min_chunk_size is not None:
            self.chunker.min_chunk_size = min_chunk_size
        if context_window is not None:
            self.chunker.context_window = context_window
        
        # è¿”å›å½“å‰å‚æ•°ï¼Œç”¨äºæ˜¾ç¤º
        return {
            'max_chunk_size': self.chunker.max_chunk_size,
            'min_chunk_size': self.chunker.min_chunk_size,
            'context_window': self.chunker.context_window
        }
    
    def _setup_embedding_function(self):
        """å¯ç”¨GPUåŠ é€Ÿçš„embedding"""
        try:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            st.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
            
            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name=self.embedding_model,
                device=device  # å…³é”®ï¼šæŒ‡å®šGPU
            )
        except Exception as e:
            st.warning(f"GPUåŠ é€Ÿå¤±è´¥ï¼Œä½¿ç”¨CPU: {e}")
    
    def _initialize_collection(self):
        """åˆå§‹åŒ–collection"""
        try:
            # å°è¯•è·å–ç°æœ‰collection
            self.collection = self.client.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function
            )
        except:
            # åˆ›å»ºæ–°collection
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"description": f"å¤æ–‡å‘é‡æ•°æ®åº“ - {self.embedding_model}"}
            )

    def change_embedding_model(self, new_model: str):
        """æ›´æ¢embeddingæ¨¡å‹å¹¶é‡å»ºç´¢å¼•"""
        st.info(f"ğŸ”„ åˆ‡æ¢embeddingæ¨¡å‹: {self.embedding_model} â†’ {new_model}")
        
        # ä¿å­˜å½“å‰æ•°æ®
        old_segments = self.segments.copy()
        
        # åˆ é™¤æ—§collection
        try:
            self.client.delete_collection(self.collection_name)
        except:
            pass
        
        # æ›´æ–°æ¨¡å‹
        self.embedding_model = new_model
        self._setup_embedding_function()
        self._initialize_collection()
        
        # é‡å»ºç´¢å¼•
        if old_segments:
            self.segments = old_segments
            self._build_vector_database()
            st.success(f"âœ… å·²åˆ‡æ¢åˆ° {new_model} å¹¶é‡å»ºç´¢å¼•")

    def load_from_directory(self, root_dir: str, file_extensions: List[str] = None) -> int:
        """é€’å½’åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰å¤æ–‡æ•°æ®"""
        if file_extensions is None:
            file_extensions = ['.txt', '.md', '.text']
        
        root_path = Path(root_dir)
        total_segments = 0
        
        if not root_path.exists():
            st.error(f"ç›®å½•ä¸å­˜åœ¨: {root_dir}")
            return 0
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰æŒ‡å®šæ ¼å¼çš„æ–‡ä»¶
        progress_bar = st.progress(0, text="æ­£åœ¨æ‰«ææ–‡ä»¶...")
        
        all_text_files = []
        for ext in file_extensions:
            pattern = f"**/*{ext}"
            files = list(root_path.glob(pattern))
            all_text_files.extend(files)
        
        # å»é‡å¹¶æ’åº
        all_text_files = sorted(list(set(all_text_files)))
        
        if not all_text_files:
            st.warning(f"åœ¨æŒ‡å®šç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(file_extensions)}")
            return 0
        
        st.info(f"å‘ç° {len(all_text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        
        # åˆ›å»ºå¤„ç†ç»Ÿè®¡
        processing_stats = {
            'processed_files': 0,
            'empty_files': 0,
            'error_files': 0,
            'total_segments': 0
        }
        
        # å¤„ç†æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶
        for file_idx, text_file in enumerate(all_text_files):
            progress_bar.progress(
                file_idx / len(all_text_files), 
                text=f"æ­£åœ¨å¤„ç†: {text_file.name} ({file_idx + 1}/{len(all_text_files)})"
            )
            
            try:
                # è§£ææ–‡ä»¶è·¯å¾„ä»¥è·å–ä¹¦åå’Œç¯‡ç« ä¿¡æ¯
                book_name, chapter_name = self._parse_file_path(text_file, root_path)
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                content = self._read_text_file(text_file)
                
                if content and len(content.strip()) > 10:  # å¿½ç•¥è¿‡çŸ­çš„æ–‡ä»¶
                    segments = self._process_chapter(book_name, chapter_name, content, str(text_file))
                    self.segments.extend(segments)
                    total_segments += len(segments)
                    processing_stats['total_segments'] += len(segments)
                    processing_stats['processed_files'] += 1
                    
                    # å®æ—¶æ˜¾ç¤ºå¤„ç†ç»“æœ
                    if len(segments) > 0:
                        st.sidebar.success(f"âœ… {book_name}/{chapter_name}: {len(segments)} æ®µ")
                else:
                    processing_stats['empty_files'] += 1
                    st.sidebar.warning(f"âš ï¸ ç©ºæ–‡ä»¶: {text_file.name}")
                    
            except Exception as e:
                processing_stats['error_files'] += 1
                st.sidebar.error(f"âŒ å¤„ç†å¤±è´¥: {text_file.name} - {str(e)}")
        
        # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
        st.sidebar.markdown("### ğŸ“Š å¤„ç†ç»Ÿè®¡")
        st.sidebar.metric("æˆåŠŸå¤„ç†", processing_stats['processed_files'])
        st.sidebar.metric("ç©ºæ–‡ä»¶", processing_stats['empty_files'])
        st.sidebar.metric("é”™è¯¯æ–‡ä»¶", processing_stats['error_files'])
        st.sidebar.metric("æ€»æ–‡æœ¬æ®µ", processing_stats['total_segments'])
        
        # å®Œæˆæ•°æ®åŠ è½½
        progress_bar.progress(1.0, text="æ„å»ºå‘é‡æ•°æ®åº“...")
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        if self.segments:
            self._build_vector_database()
            progress_bar.progress(1.0, text=f"åŠ è½½å®Œæˆï¼å…±å¤„ç† {total_segments} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        else:
            progress_bar.progress(1.0, text="æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®")
        
        return total_segments
    
    def load_single_directory(self, root_dir: str, file_extensions: List[str] = None) -> int:
        """åŠ è½½å•ä¸ªç›®å½•ï¼ˆéé€’å½’ï¼‰çš„å¤æ–‡æ•°æ®"""
        if file_extensions is None:
            file_extensions = ['.txt', '.md', '.text']
        
        root_path = Path(root_dir)
        total_segments = 0
        
        if not root_path.exists():
            st.error(f"ç›®å½•ä¸å­˜åœ¨: {root_dir}")
            return 0
        
        # æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶
        progress_bar = st.progress(0, text="æ­£åœ¨æ‰«ææ–‡ä»¶...")
        
        all_text_files = []
        for ext in file_extensions:
            files = list(root_path.glob(f"*{ext}"))
            all_text_files.extend(files)
        
        # ä¹Ÿæ£€æŸ¥ç›´æ¥å­ç›®å½•ä¸­çš„æ–‡ä»¶
        for subdir in root_path.iterdir():
            if subdir.is_dir():
                for ext in file_extensions:
                    files = list(subdir.glob(f"*{ext}"))
                    all_text_files.extend(files)
        
        all_text_files = sorted(list(set(all_text_files)))
        
        if not all_text_files:
            st.warning(f"åœ¨æŒ‡å®šç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(file_extensions)}")
            return 0
        
        st.info(f"å‘ç° {len(all_text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        
        # å¤„ç†æ–‡ä»¶çš„é€»è¾‘ä¸é€’å½’ç‰ˆæœ¬ç›¸åŒ
        for file_idx, text_file in enumerate(all_text_files):
            progress_bar.progress(
                file_idx / len(all_text_files), 
                text=f"æ­£åœ¨å¤„ç†: {text_file.name} ({file_idx + 1}/{len(all_text_files)})"
            )
            
            try:
                book_name, chapter_name = self._parse_file_path(text_file, root_path)
                content = self._read_text_file(text_file)
                
                if content and len(content.strip()) > 10:
                    segments = self._process_chapter(book_name, chapter_name, content, str(text_file))
                    self.segments.extend(segments)
                    total_segments += len(segments)
                    
                    if len(segments) > 0:
                        st.sidebar.success(f"âœ… {book_name}/{chapter_name}: {len(segments)} æ®µ")
                        
            except Exception as e:
                st.sidebar.error(f"âŒ å¤„ç†å¤±è´¥: {text_file.name} - {str(e)}")
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        if self.segments:
            progress_bar.progress(1.0, text="æ„å»ºå‘é‡æ•°æ®åº“...")
            self._build_vector_database()
        
        progress_bar.progress(1.0, text=f"åŠ è½½å®Œæˆï¼å…±å¤„ç† {total_segments} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        return total_segments
    
    def _parse_file_path(self, file_path: Path, root_path: Path) -> Tuple[str, str]:
        """æ™ºèƒ½è§£ææ–‡ä»¶è·¯å¾„ä»¥è·å–ä¹¦åå’Œç¯‡ç« ä¿¡æ¯"""
        try:
            # è·å–ç›¸å¯¹äºæ ¹ç›®å½•çš„è·¯å¾„
            relative_path = file_path.relative_to(root_path)
            path_parts = list(relative_path.parts[:-1])  # æ’é™¤æ–‡ä»¶å
            
            # æ ¹æ®ç›®å½•å±‚æ¬¡æ™ºèƒ½è§£æ
            if len(path_parts) == 0:
                # ç›´æ¥åœ¨æ ¹ç›®å½•ï¼štext.txt
                book_name = root_path.name
                chapter_name = file_path.stem
            elif len(path_parts) == 1:
                # ä¸€å±‚ç›®å½•ï¼šä¹¦å/text.txt æˆ– ç¯‡ç« /text.txt
                book_name = path_parts[0]
                chapter_name = file_path.stem if file_path.stem != 'text' else path_parts[0]
            elif len(path_parts) == 2:
                # æ ‡å‡†ç»“æ„ï¼šä¹¦å/ç¯‡ç« /text.txt
                book_name = path_parts[0]
                chapter_name = path_parts[1]
            else:
                # æ·±å±‚ç»“æ„ï¼šä¹¦å/å·/ç¯‡ç« /å­ç« èŠ‚/text.txt
                book_name = path_parts[0]
                # å°†ä¸­é—´å±‚çº§ç”¨å±‚æ¬¡åˆ†éš”ç¬¦è¿æ¥
                chapter_parts = path_parts[1:]
                chapter_name = " > ".join(chapter_parts)
            
            # æ¸…ç†åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            book_name = self._clean_name(book_name)
            chapter_name = self._clean_name(chapter_name)
            
            return book_name, chapter_name
            
        except Exception as e:
            # å‡ºé”™æ—¶ä½¿ç”¨é»˜è®¤å€¼
            return root_path.name, file_path.stem
    
    def _clean_name(self, name: str) -> str:
        """æ¸…ç†æ–‡ä»¶/ç›®å½•åç§°"""
        # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€
        name = re.sub(r'^\d+[-._]\s*', '', name)
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        name = re.sub(r'[_-]+', ' ', name)
        return name.strip()
    
    def _process_chapter(self, book: str, chapter: str, content: str, file_path: str = "") -> List[AncientTextSegment]:
        """å¤„ç†å•ä¸ªç¯‡ç« ï¼ˆä½¿ç”¨æ–°çš„å±‚çº§åˆ†å—ï¼‰"""
        segments = []
        
        # ä½¿ç”¨æ–°çš„åˆ†å—æ–¹æ³•
        chunks = self.chunker.chunk_text(content)
        
        for chunk in chunks:
            # åˆ†ææ–‡æœ¬
            topic = self.analyzer.classify_topic(chunk.content)
            
            # åˆ›å»ºç‰‡æ®µID
            segment_id = f"{book}_{chapter}_{chunk.paragraph_index:03d}_{chunk.sub_index:02d}"
            
            # æ•´åˆå…ƒæ•°æ®
            metadata = {
                'length': len(chunk.content),
                'paragraph_index': chunk.paragraph_index,
                'sub_index': chunk.sub_index,
                'is_continuation': chunk.is_continuation,
                'prev_context': chunk.prev_context,
                'next_context': chunk.next_context,
                'classical_terms': self._extract_classical_terms(chunk.content),
            }
            
            # å¦‚æœchunkæœ‰é¢å¤–çš„metadataï¼Œåˆå¹¶è¿›æ¥
            if chunk.metadata:
                metadata.update(chunk.metadata)
            
            # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            full_context = f"{chunk.prev_context} **{chunk.content}** {chunk.next_context}"
            
            segment = AncientTextSegment(
                book=book,
                chapter=chapter,
                content=chunk.content,
                topic=topic,
                segment_id=segment_id,
                context=full_context,
                metadata=metadata
            )
            segments.append(segment)
        
        return segments
    
    def _extract_classical_terms(self, text: str) -> List[str]:
        """æå–å¤å…¸æœ¯è¯­"""
        classical_terms = [
            'ä»', 'ä¹‰', 'ç¤¼', 'æ™º', 'ä¿¡', 'å¾·', 'é“', 'å¤©', 'å›å­', 'å°äºº',
            'å­¦', 'ä¹ ', 'æ•™', 'è¯²', 'æ”¿', 'æ²»', 'æ°‘', 'å›½', 'å®¶', 'å­',
            'æ‚Œ', 'å¿ ', 'æ•', 'è¯š', 'æ­£', 'ä¿®', 'é½', 'æ²»', 'å¹³'
        ]
        
        found_terms = []
        for term in classical_terms:
            if term in text:
                found_terms.append(term)
        
        return found_terms
    
    def _read_text_file(self, file_path: Path) -> str:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # å¤„ç†å¯èƒ½çš„XMLæ ‡ç­¾
            content_match = re.search(r'<content>(.*?)</content>', content, re.DOTALL)
            if content_match:
                return content_match.group(1).strip()
            
            return content
        except Exception as e:
            st.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    
    
    def _build_vector_database(self):
        """æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆä¿®å¤åˆ é™¤é”™è¯¯ï¼‰"""
        # æ¸…ç©ºæ—§æ•°æ® - ä¿®å¤ç‰ˆæœ¬
        try:
            if self.collection.count() > 0:
                # æ–¹æ³•1ï¼šè·å–æ‰€æœ‰IDç„¶ååˆ é™¤
                all_data = self.collection.get()
                if all_data['ids']:
                    self.collection.delete(ids=all_data['ids'])
            
            
        except Exception as e:
            # å¦‚æœåˆ é™¤å¤±è´¥ï¼Œå°è¯•é‡æ–°åˆ›å»ºcollection
            st.warning(f"æ¸…ç©ºæ•°æ®åº“æ—¶å‡ºç°é—®é¢˜ï¼Œå°†é‡æ–°åˆ›å»ºï¼š{e}")
            try:
                self.client.delete_collection(self.collection_name)
            except:
                pass
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "é€šç”¨å¤æ–‡å‘é‡æ•°æ®åº“"}
            )
        
        # æ‰¹é‡æ·»åŠ 
        batch_size = 5000
        for i in range(0, len(self.segments), batch_size):
            batch_segments = self.segments[i:i+batch_size]
            
            documents = [seg.content for seg in batch_segments]
            
            # è¿‡æ»¤å¹¶è½¬æ¢å…ƒæ•°æ®ï¼Œç¡®ä¿åªåŒ…å«åŸºæœ¬æ•°æ®ç±»å‹
            metadatas = []
            for seg in batch_segments:
                # åˆ›å»ºåŸºç¡€å…ƒæ•°æ®
                base_metadata = {
                    'book': seg.book,
                    'chapter': seg.chapter,
                    'topic': seg.topic,
                    'segment_id': seg.segment_id
                }
                
                # å¤„ç†æ‰©å±•å…ƒæ•°æ®
                if hasattr(seg, 'metadata') and seg.metadata:
                    filtered_extended = self._filter_metadata_for_chromadb(seg.metadata)
                    base_metadata.update(filtered_extended)
                
                metadatas.append(base_metadata)
            
            ids = [seg.segment_id for seg in batch_segments]
            
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )

        if self.segments:
            # ç¡®ä¿æ­£ç¡®è·å–ç”¨æˆ·é…ç½®
            use_reranker = st.session_state.get('use_reranker', HAS_RERANKER)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            st.info(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ£€ç´¢å™¨ï¼Œé‡æ’åº: {'å¯ç”¨' if use_reranker else 'ç¦ç”¨'}")
            
            self.retriever = HybridRetriever(
                self.collection, 
                self.segments, 
                # use_reranker=use_reranker  # ç¡®ä¿æ­£ç¡®ä¼ é€’å‚æ•°
            )

            # æ˜¾ç¤ºé‡æ’åºçŠ¶æ€
            st.info("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
            
    
    def _filter_metadata_for_chromadb(self, metadata: Dict) -> Dict:
        """è¿‡æ»¤å…ƒæ•°æ®ï¼Œç¡®ä¿åªåŒ…å«ChromaDBæ”¯æŒçš„æ•°æ®ç±»å‹"""
        filtered = {}
        
        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool)):
                # åŸºæœ¬ç±»å‹ç›´æ¥ä¿ç•™
                filtered[key] = value
            elif isinstance(value, list):
                # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²å’Œè®¡æ•°
                if value:  # éç©ºåˆ—è¡¨
                    filtered[f"{key}_str"] = ', '.join(map(str, value))
                    filtered[f"{key}_count"] = len(value)
                else:
                    filtered[f"{key}_count"] = 0
            elif isinstance(value, dict):
                # å°†å­—å…¸è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                filtered[f"{key}_str"] = str(value)
            else:
                # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                filtered[f"{key}_str"] = str(value)
        
        return filtered
    
    # rag_system.py ä¸­ä¿®æ”¹

    def search(self, query: str, top_k: int = 5, 
            search_mode: str = 'hybrid',
            metadata_filter: Dict[str, str] = None,
            bm25_weight: float = None,
            vector_weight: float = None) -> List[Dict[str, Any]]:
        """
        ç»Ÿä¸€çš„æœç´¢æ¥å£
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›ç»“æœæ•°é‡
            search_mode: æ£€ç´¢æ¨¡å¼ ('hybrid', 'vector', 'bm25')
            metadata_filter: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            bm25_weight: BM25æƒé‡
            vector_weight: å‘é‡æƒé‡
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.retriever:
            st.error("æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåŠ è½½æ•°æ®")
            return []
        
        try:
            # å®šä¹‰æœç´¢æ–¹æ³•æ˜ å°„
            search_methods = {
                'hybrid': {
                    'with_filter': self.retriever.hybrid_search_with_filter,
                    'without_filter': self.retriever.hybrid_search,
                    'fallback': self.retriever._vector_search
                },
                'vector': {
                    'with_filter': self.retriever.vector_search_with_filter,
                    'without_filter': self.retriever._vector_search,
                    'fallback': None
                },
                'bm25': {
                    'with_filter': self.retriever.bm25_search_with_filter,
                    'without_filter': self.retriever._bm25_search,
                    'fallback': None
                }
            }
            
            # è·å–å¯¹åº”çš„æœç´¢æ–¹æ³•
            mode_methods = search_methods.get(search_mode)
            if not mode_methods:
                st.error(f"ä¸æ”¯æŒçš„æœç´¢æ¨¡å¼: {search_mode}")
                return []
            
            # ç‰¹æ®Šå¤„ç†ï¼šBM25ä¸å¯ç”¨æ—¶çš„é™çº§
            if search_mode == 'bm25' and not HAS_BM25:
                st.warning("BM25æ£€ç´¢ä¸å¯ç”¨ï¼Œè¯·å®‰è£… rank-bm25 åŒ…")
                return []
            
            if search_mode == 'hybrid' and not HAS_BM25:
                st.info("BM25ä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å‘é‡æ£€ç´¢æ¨¡å¼")
                # ä½¿ç”¨é™çº§æ–¹æ³•
                if metadata_filter:
                    return self.retriever.vector_search_with_filter(
                        query, top_k, metadata_filter
                    )
                else:
                    return mode_methods['fallback'](query, top_k)
            
            # æ‰§è¡Œæœç´¢
            if metadata_filter:
                # æœ‰è¿‡æ»¤æ¡ä»¶
                return mode_methods['with_filter'](
                    query, top_k, metadata_filter,
                    bm25_weight=bm25_weight,
                    vector_weight=vector_weight
                )
            else:
                # æ— è¿‡æ»¤æ¡ä»¶
                method = mode_methods['without_filter']
                if search_mode == 'hybrid':
                    # hybridæ¨¡å¼éœ€è¦ä¼ é€’æƒé‡å‚æ•°
                    return method(
                        query, top_k,
                        bm25_weight=bm25_weight,
                        vector_weight=vector_weight
                    )
                else:
                    # å…¶ä»–æ¨¡å¼ä¸éœ€è¦æƒé‡å‚æ•°
                    return method(query, top_k)
                
        except Exception as e:
            st.error(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return []

    # åˆ é™¤åŸæ¥çš„search_with_metadataæ–¹æ³•ï¼Œæ”¹ä¸ºåˆ«åæˆ–å‘åå…¼å®¹
    def search_with_metadata(self, query: str, top_k: int = 5, 
                            search_mode: str = 'hybrid', 
                            metadata_filter: Dict[str, str] = None,
                            bm25_weight: float = None,
                            vector_weight: float = None) -> List[Dict[str, Any]]:
        """å‘åå…¼å®¹çš„æ–¹æ³•ï¼Œè°ƒç”¨ç»Ÿä¸€çš„searchæ–¹æ³•"""
        return self.search(
            query=query,
            top_k=top_k,
            search_mode=search_mode,
            metadata_filter=metadata_filter,
            bm25_weight=bm25_weight,
            vector_weight=vector_weight
        )
    

    def analyze_and_search(self, query: str, top_k: int = 5, 
                  search_mode: str = 'hybrid',
                  metadata_filter: Dict[str, str] = None) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
        """
        ç»¼åˆåˆ†æå¹¶æ‰§è¡Œæœç´¢
        è¿”å›ï¼š(åˆ†æç»“æœ, æœç´¢ç»“æœ)
        """
        # å¦‚æœæ²¡æœ‰APIå®¢æˆ·ç«¯ï¼Œç›´æ¥æ‰§è¡Œæœç´¢
        if not self.api_client:
            analysis_result = {
                'need_search': True,
                'reason': 'æœªé…ç½®APIå®¢æˆ·ç«¯',
                'optimized_query': query,
                'bm25_weight': 0.3,
                'vector_weight': 0.7,
                'direct_answer': None,
                'extracted_book': None
            }
            search_results = self.search(query, top_k, search_mode, metadata_filter)
            return analysis_result, search_results
        
        # è°ƒç”¨ç»¼åˆåˆ†æAPI
        analysis_result = self.api_client.analyze_query(query)
        
        # å¦‚æœä¸éœ€è¦æœç´¢ï¼Œè¿”å›ç©ºç»“æœ
        if not analysis_result['need_search']:
            return analysis_result, []
        
        # --- æ–°å¢é€»è¾‘ï¼šåˆå¹¶AIæå–çš„è¿‡æ»¤å™¨å’Œç”¨æˆ·æ‰‹åŠ¨è¾“å…¥çš„è¿‡æ»¤å™¨ ---
        # å¤åˆ¶ç”¨æˆ·æ‰‹åŠ¨è¾“å…¥çš„è¿‡æ»¤å™¨ï¼Œé¿å…ç›´æ¥ä¿®æ”¹
        effective_metadata_filter = metadata_filter.copy() if metadata_filter else {}
        
        extracted_book = analysis_result.get('extracted_book')
        
        # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœAIæå–äº†ä¹¦åï¼Œå¹¶ä¸”ç”¨æˆ·æ²¡æœ‰æ‰‹åŠ¨æŒ‡å®šä¹¦åï¼Œåˆ™ä½¿ç”¨AIæå–çš„ä¹¦å
        if extracted_book and 'book' not in effective_metadata_filter:
            effective_metadata_filter['book'] = extracted_book
            # æ·»åŠ ä¸€ä¸ªæ ‡å¿—ï¼Œæ–¹ä¾¿UIæ˜¾ç¤ºæç¤ºä¿¡æ¯
            analysis_result['filter_source'] = 'ai'

        # ä½¿ç”¨ä¼˜åŒ–åçš„æŸ¥è¯¢å’Œæƒé‡è¿›è¡Œæœç´¢
        optimized_query = analysis_result['optimized_query']
        bm25_weight = analysis_result['bm25_weight']
        vector_weight = analysis_result['vector_weight']
        
        # æ‰§è¡Œæœç´¢ï¼Œä¼ é€’åˆå¹¶åçš„è¿‡æ»¤å™¨
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å°† effective_metadata_filter ä¼ é€’ç»™æœç´¢å‡½æ•°
        search_results = self.search(
            optimized_query, top_k, search_mode,
            metadata_filter=effective_metadata_filter, # --- ä¿®æ”¹ ---
            bm25_weight=bm25_weight,
            vector_weight=vector_weight
        )
        
        return analysis_result, search_results
    
    # åœ¨ UniversalAncientRAG ç±»ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•
    def multi_round_search(self, query: str, top_k_per_round: int = 5, 
                        search_mode: str = 'hybrid') -> Dict[str, Any]:
        """
        æ‰§è¡Œå¤šè½®æ£€ç´¢
        è¿”å›ï¼š{
            'original_query': str,
            'decomposition': dict,  # æ‹†è§£ç»“æœ
            'subtasks_results': list,  # å„å­ä»»åŠ¡çš„æ£€ç´¢ç»“æœ
            'synthesis': str  # ç»¼åˆå›ç­”
        }
        """
        # å¦‚æœæ²¡æœ‰APIå®¢æˆ·ç«¯ï¼Œé™çº§ä¸ºå•è½®æ£€ç´¢
        if not self.api_client:
            st.warning("å¤šè½®æ£€ç´¢éœ€è¦é…ç½®APIå®¢æˆ·ç«¯")
            single_results = self.search(query, top_k_per_round * 2, search_mode)
            return {
                'original_query': query,
                'decomposition': {'need_multi_round': False, 'reason': 'æ— APIå®¢æˆ·ç«¯'},
                'subtasks_results': [{'subtask_query': query, 'results': single_results}],
                'synthesis': self._generate_basic_answer(query, single_results)
            }
        
        # ç¬¬ä¸€æ­¥ï¼šä»»åŠ¡æ‹†è§£
        with st.spinner("ğŸ” æ­£åœ¨åˆ†æé—®é¢˜å¤æ‚åº¦..."):
            decomposition = self.api_client.decompose_complex_query(query)
        
        # å¦‚æœä¸éœ€è¦å¤šè½®æ£€ç´¢ï¼Œæ‰§è¡Œå•è½®æ£€ç´¢
        if not decomposition.get('need_multi_round', False):
            st.info(f"ğŸ’¡ {decomposition.get('reason', 'è¯¥é—®é¢˜ä¸éœ€è¦å¤šè½®æ£€ç´¢')}")
            single_results = self.search(query, top_k_per_round * 2, search_mode)
            return {
                'original_query': query,
                'decomposition': decomposition,
                'subtasks_results': [{'subtask_query': query, 'results': single_results}],
                'synthesis': self.generate_answer(query, single_results)
            }
        
        # ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œå¤šè½®æ£€ç´¢
        st.success(f"âœ… å·²å°†é—®é¢˜æ‹†è§£ä¸º {len(decomposition['subtasks'])} ä¸ªå­ä»»åŠ¡")
        
        subtasks_results = []
        progress_bar = st.progress(0, text="å¼€å§‹å¤šè½®æ£€ç´¢...")
        
        for idx, subtask in enumerate(decomposition['subtasks']):
            progress = (idx + 1) / len(decomposition['subtasks'])
            progress_bar.progress(
                progress, 
                text=f"æ­£åœ¨æ£€ç´¢å­ä»»åŠ¡ {idx+1}: {subtask['subtask_focus']}"
            )
            
            # å¯¹æ¯ä¸ªå­ä»»åŠ¡æ‰§è¡Œæ£€ç´¢
            with st.expander(f"å­ä»»åŠ¡ {idx+1}: {subtask['subtask_query']}", expanded=False):
                # ä½¿ç”¨å­ä»»åŠ¡ç‰¹å®šçš„æƒé‡
                weights = subtask.get('search_weight', {'bm25': 0.3, 'vector': 0.7})
                
                results = self.search(
                    subtask['subtask_query'],
                    top_k_per_round,
                    search_mode,
                    bm25_weight=weights['bm25'],
                    vector_weight=weights['vector']
                )
                
                # æ˜¾ç¤ºå­ä»»åŠ¡ç»“æœ
                if results:
                    st.write(f"ğŸ¯ æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç‰‡æ®µ")
                    for i, res in enumerate(results[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
                        meta = res['metadata']
                        st.write(f"{i+1}. ã€Š{meta['book']}Â·{meta['chapter']}ã€‹")
                
                subtasks_results.append({
                    'subtask_id': subtask['subtask_id'],
                    'subtask_query': subtask['subtask_query'],
                    'subtask_focus': subtask['subtask_focus'],
                    'results': results,
                    'synthesis_instruction': decomposition.get('synthesis_instruction', '')
                })
        
        progress_bar.progress(1.0, text="æ£€ç´¢å®Œæˆï¼Œæ­£åœ¨ç»¼åˆåˆ†æ...")
        
        # ç¬¬ä¸‰æ­¥ï¼šç»¼åˆå¤šè½®ç»“æœ
        with st.spinner("ğŸ¤– æ­£åœ¨ç»¼åˆå¤šè½®æ£€ç´¢ç»“æœ..."):
            synthesis = self.api_client.synthesize_multi_round_results(query, subtasks_results)
        
        return {
            'original_query': query,
            'decomposition': decomposition,
            'subtasks_results': subtasks_results,
            'synthesis': synthesis
        }
    
    
    def generate_answer(self, query: str, context: list) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        if self.api_client:
            return self.api_client.generate_answer(query, context)
        return self._generate_basic_answer(query, context)
    
    
    def _generate_basic_answer(self, query: str, context: List[Dict[str, Any]]) -> str:
        """ç”ŸæˆåŸºç¡€å›ç­”"""
        answer = f"å…³äºã€Œ{query}ã€ï¼Œåœ¨å¤æ–‡ä¸­æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n\n"
        
        for i, item in enumerate(context[:3], 1):
            meta = item['metadata']
            
            # æ™ºèƒ½é€‰æ‹©æ˜¾ç¤ºåˆ†æ•°
            score_info = self._get_score_info(item)
            
            answer += f"**{i}. ã€Š{meta['book']}Â·{meta['chapter']}ã€‹**\n"
            answer += f"åŸæ–‡ï¼šã€Œ{item['content']}ã€\n"
            answer += f"è¯é¢˜ï¼š{meta['topic']} | {score_info}\n\n"
    
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        books = set(item['metadata']['book'] for item in context)
        topics = set(item['metadata']['topic'] for item in context)
        
        answer += f"ğŸ’¡ **å†…å®¹åˆ†æ**ï¼š\n"
        answer += f"- æ¶‰åŠå…¸ç±ï¼š{', '.join(books)}\n"
        answer += f"- ç›¸å…³ä¸»é¢˜ï¼š{', '.join(topics)}\n"
        answer += f"- æ£€ç´¢æ¨¡å¼ï¼šæ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ç›¸ä¼¼åº¦ï¼‰"
        
        return answer
    
    def _get_score_info(self, item: Dict[str, Any]) -> str:
        """è·å–åˆ†æ•°æ˜¾ç¤ºä¿¡æ¯"""
        # ä¼˜å…ˆæ˜¾ç¤ºé‡æ’åºåˆ†æ•°
        if 'rerank_score' in item and item['rerank_score'] > 0:
            return f"é‡æ’åºè¯„åˆ†ï¼š{item['rerank_score']:.3f}"
        
        # å…¶æ¬¡æ˜¾ç¤ºç»¼åˆåˆ†æ•°
        elif 'combined_score' in item and item['combined_score'] > 0:
            return f"ç»¼åˆè¯„åˆ†ï¼š{item['combined_score']:.3f}"
        
        # æ˜¾ç¤ºå‘é‡åˆ†æ•°
        elif 'vector_score' in item and item['vector_score'] > 0:
            return f"ç›¸ä¼¼åº¦ï¼š{item['vector_score']:.3f}"
        
        # æ˜¾ç¤ºBM25åˆ†æ•°
        elif 'bm25_score' in item and item['bm25_score'] > 0:
            return f"åŒ¹é…åº¦ï¼š{item['bm25_score']:.3f}"
        
        # é»˜è®¤æ˜¾ç¤º
        else:
            return "ç›¸å…³åº¦ï¼šé«˜"
    
    def save_processing_results(self, output_dir: str = "./processing_results") -> bool:
        """ä¿å­˜å¤„ç†ç»“æœåˆ°æŒ‡å®šç›®å½•"""
        if not self.segments:
            st.warning("æ²¡æœ‰å¤„ç†ç»“æœå¯ä¿å­˜")
            return False
        
        try:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. ä¿å­˜è¯¦ç»†çš„JSONæ ¼å¼ç»“æœ
            self._save_detailed_json(output_path, timestamp)
            
            # 2. ä¿å­˜ä¾¿äºæŸ¥çœ‹çš„HTMLæ ¼å¼ç»“æœ
            self._save_html_report(output_path, timestamp)
            
            # 3. ä¿å­˜CSVæ ¼å¼ç»“æœï¼ˆä¾¿äºExcelæ‰“å¼€ï¼‰
            self._save_csv_report(output_path, timestamp)
            
            # 4. ä¿å­˜æŒ‰ä¹¦ç±åˆ†ç±»çš„æ–‡æœ¬æ–‡ä»¶
            self._save_by_books(output_path, timestamp)
            
            # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            self._save_statistics_report(output_path, timestamp)
            
            st.success(f"âœ… å¤„ç†ç»“æœå·²ä¿å­˜åˆ°ï¼š{output_path.absolute()}")
            return True
            
        except Exception as e:
            st.error(f"ä¿å­˜å¤„ç†ç»“æœå¤±è´¥ï¼š{str(e)}")
            return False
    
    def _save_detailed_json(self, output_path: Path, timestamp: str):
        """ä¿å­˜è¯¦ç»†çš„JSONæ ¼å¼ç»“æœ"""
        json_file = output_path / f"segments_detailed_{timestamp}.json"
        
        segments_data = []
        for seg in self.segments:
            segment_dict = asdict(seg)
            segments_data.append(segment_dict)
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(segments_data, f, ensure_ascii=False, indent=2)
        
        st.info(f"ğŸ“„ è¯¦ç»†JSONç»“æœï¼š{json_file.name}")
    
    def _save_html_report(self, output_path: Path, timestamp: str):
        """ä¿å­˜HTMLæ ¼å¼çš„å¯è§†åŒ–æŠ¥å‘Š"""
        html_file = output_path / f"processing_report_{timestamp}.html"
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å¤æ–‡å¤„ç†ç»“æœæŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 20px; margin-bottom: 30px; }}
        .stats {{ display: flex; justify-content: space-around; margin-bottom: 30px; }}
        .stat-card {{ background: #ecf0f1; padding: 15px; border-radius: 8px; text-align: center; min-width: 120px; }}
        .stat-number {{ font-size: 24px; font-weight: bold; color: #e74c3c; }}
        .segment {{ border: 1px solid #bdc3c7; margin-bottom: 15px; border-radius: 8px; overflow: hidden; }}
        .segment-header {{ background: #34495e; color: white; padding: 10px; font-weight: bold; }}
        .segment-content {{ padding: 15px; }}
        .metadata {{ background: #f8f9fa; padding: 10px; margin-top: 10px; border-radius: 4px; font-size: 0.9em; }}
        .content-text {{ line-height: 1.8; margin: 10px 0; font-size: 16px; }}
        .book-section {{ margin-bottom: 40px; }}
        .book-title {{ color: #8e44ad; font-size: 20px; font-weight: bold; margin-bottom: 15px; border-left: 4px solid #8e44ad; padding-left: 10px; }}
        .context {{ background: #fff3cd; padding: 10px; margin-top: 10px; border-radius: 4px; border-left: 4px solid #ffc107; }}
        .topic-tag {{ background: #17a2b8; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.8em; margin-right: 5px; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“œ å¤æ–‡å¤„ç†ç»“æœæŠ¥å‘Š</h1>
            <p>ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number">{len(self.segments)}</div>
                <div>æ–‡æœ¬ç‰‡æ®µæ€»æ•°</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{len(set(seg.book for seg in self.segments))}</div>
                <div>ä¹¦ç±æ•°é‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{len(set(seg.chapter for seg in self.segments))}</div>
                <div>ç« èŠ‚æ•°é‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{sum(len(seg.content) for seg in self.segments)}</div>
                <div>æ€»å­—ç¬¦æ•°</div>
            </div>
        </div>
        
        {self._generate_html_segments_by_book()}
    </div>
</body>
</html>"""
        
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        st.info(f"ğŸŒ HTMLå¯è§†åŒ–æŠ¥å‘Šï¼š{html_file.name}")
    
    def _generate_html_segments_by_book(self) -> str:
        """ç”ŸæˆæŒ‰ä¹¦ç±åˆ†ç»„çš„HTMLå†…å®¹"""
        html_parts = []
        
        # æŒ‰ä¹¦ç±åˆ†ç»„
        books = {}
        for seg in self.segments:
            if seg.book not in books:
                books[seg.book] = []
            books[seg.book].append(seg)
        
        for book_name, segments in books.items():
            html_parts.append(f'<div class="book-section">')
            html_parts.append(f'<div class="book-title">ğŸ“– {book_name} ({len(segments)} ä¸ªç‰‡æ®µ)</div>')
            
            for i, seg in enumerate(segments, 1):
                html_parts.append(f'''
                <div class="segment">
                    <div class="segment-header">
                        ç‰‡æ®µ {i} - {seg.chapter} 
                        <span class="topic-tag">{seg.topic}</span>
                    </div>
                    <div class="segment-content">
                        <div class="content-text">{seg.content}</div>
                        <div class="metadata">
                            <strong>ç‰‡æ®µIDï¼š</strong>{seg.segment_id} | 
                            <strong>å­—ç¬¦æ•°ï¼š</strong>{len(seg.content)} | 
                            <strong>ä½ç½®ï¼š</strong>{seg.metadata.get('position', 'æœªçŸ¥')}
                        </div>
                        {f'<div class="context"><strong>ä¸Šä¸‹æ–‡ï¼š</strong><br>{seg.context}</div>' if seg.context != seg.content else ''}
                    </div>
                </div>
                ''')
            
            html_parts.append('</div>')
        
        return ''.join(html_parts)
    
    def _save_csv_report(self, output_path: Path, timestamp: str):
        """ä¿å­˜CSVæ ¼å¼æŠ¥å‘Šï¼ˆä¾¿äºExcelæŸ¥çœ‹ï¼‰"""
        csv_file = output_path / f"segments_table_{timestamp}.csv"
        
        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            headers = [
                'åºå·', 'ä¹¦å', 'ç« èŠ‚', 'è¯´è¯äºº', 'è¯é¢˜', 'å†…å®¹', 
                'å­—ç¬¦æ•°', 'ç‰‡æ®µID', 'ä½ç½®', 'æ˜¯å¦å¯¹è¯', 'å¤å…¸æœ¯è¯­'
            ]
            writer.writerow(headers)
            
            # å†™å…¥æ•°æ®
            for i, seg in enumerate(self.segments, 1):
                classical_terms = seg.metadata.get('classical_terms', [])
                if isinstance(classical_terms, list):
                    terms_str = ', '.join(classical_terms)
                else:
                    terms_str = str(classical_terms)
                
                row = [
                    i,
                    seg.book,
                    seg.chapter,
                    seg.topic,
                    seg.content,
                    len(seg.content),
                    seg.segment_id,
                    seg.metadata.get('position', ''),
                    seg.metadata.get('has_dialogue', False),
                    terms_str
                ]
                writer.writerow(row)
        
        st.info(f"ğŸ“Š CSVè¡¨æ ¼æ–‡ä»¶ï¼š{csv_file.name}")
    
    def _save_by_books(self, output_path: Path, timestamp: str):
        """æŒ‰ä¹¦ç±ä¿å­˜åˆ†ç±»çš„æ–‡æœ¬æ–‡ä»¶"""
        books_dir = output_path / f"by_books_{timestamp}"
        books_dir.mkdir(exist_ok=True)
        
        # æŒ‰ä¹¦ç±åˆ†ç»„
        books = {}
        for seg in self.segments:
            if seg.book not in books:
                books[seg.book] = {}
            if seg.chapter not in books[seg.book]:
                books[seg.book][seg.chapter] = []
            books[seg.book][seg.chapter].append(seg)
        
        for book_name, chapters in books.items():
            book_dir = books_dir / book_name
            book_dir.mkdir(exist_ok=True)
            
            # ä¸ºæ¯ä¸ªç« èŠ‚åˆ›å»ºæ–‡ä»¶
            for chapter_name, segments in chapters.items():
                chapter_file = book_dir / f"{chapter_name}_segments.txt"
                
                with open(chapter_file, 'w', encoding='utf-8') as f:
                    f.write(f"ã€Š{book_name}ã€‹- {chapter_name}\n")
                    f.write("=" * 50 + "\n\n")
                    
                    for i, seg in enumerate(segments, 1):
                        f.write(f"ã€ç‰‡æ®µ {i}ã€‘\n")
                        f.write(f"è¯é¢˜ï¼š{seg.topic}\n")
                        f.write(f"å†…å®¹ï¼š{seg.content}\n")
                        f.write(f"ç‰‡æ®µIDï¼š{seg.segment_id}\n")
                        f.write(f"å­—ç¬¦æ•°ï¼š{len(seg.content)}\n")
                        
                        # æ·»åŠ å¤å…¸æœ¯è¯­ä¿¡æ¯
                        terms = seg.metadata.get('classical_terms', [])
                        if terms:
                            if isinstance(terms, list):
                                f.write(f"å¤å…¸æœ¯è¯­ï¼š{', '.join(terms)}\n")
                            else:
                                f.write(f"å¤å…¸æœ¯è¯­ï¼š{terms}\n")
                        
                        f.write("-" * 30 + "\n\n")
        
        st.info(f"ğŸ“š æŒ‰ä¹¦ç±åˆ†ç±»çš„æ–‡ä»¶ï¼š{books_dir.name}/")
    
    def _save_statistics_report(self, output_path: Path, timestamp: str):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        stats_file = output_path / f"statistics_{timestamp}.txt"
        
        # æ”¶é›†ç»Ÿè®¡æ•°æ®
        total_segments = len(self.segments)
        books_stats = {}
        topics_stats = {}
        length_stats = []
        
        for seg in self.segments:
            # ä¹¦ç±ç»Ÿè®¡
            books_stats[seg.book] = books_stats.get(seg.book, 0) + 1
            
            # è¯é¢˜ç»Ÿè®¡
            topics_stats[seg.topic] = topics_stats.get(seg.topic, 0) + 1
            
            # é•¿åº¦ç»Ÿè®¡
            length_stats.append(len(seg.content))
        
        # è®¡ç®—é•¿åº¦ç»Ÿè®¡
        avg_length = sum(length_stats) / len(length_stats) if length_stats else 0
        min_length = min(length_stats) if length_stats else 0
        max_length = max(length_stats) if length_stats else 0
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write("å¤æ–‡å¤„ç†ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
            
            f.write("ğŸ“Š åŸºæœ¬ç»Ÿè®¡\n")
            f.write("-" * 20 + "\n")
            f.write(f"æ–‡æœ¬ç‰‡æ®µæ€»æ•°ï¼š{total_segments}\n")
            f.write(f"å¹³å‡ç‰‡æ®µé•¿åº¦ï¼š{avg_length:.1f} å­—ç¬¦\n")
            f.write(f"æœ€çŸ­ç‰‡æ®µé•¿åº¦ï¼š{min_length} å­—ç¬¦\n")
            f.write(f"æœ€é•¿ç‰‡æ®µé•¿åº¦ï¼š{max_length} å­—ç¬¦\n")
            f.write(f"æ€»å­—ç¬¦æ•°ï¼š{sum(length_stats)}\n\n")
            
            f.write("ğŸ“š ä¹¦ç±åˆ†å¸ƒ\n")
            f.write("-" * 20 + "\n")
            for book, count in sorted(books_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_segments) * 100
                f.write(f"{book}ï¼š{count} æ®µ ({percentage:.1f}%)\n")
            f.write("\n")
            
            f.write("ğŸ·ï¸ è¯é¢˜åˆ†å¸ƒ\n")
            f.write("-" * 20 + "\n")
            for topic, count in sorted(topics_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_segments) * 100
                f.write(f"{topic}ï¼š{count} æ®µ ({percentage:.1f}%)\n")
            f.write("\n")
        
        st.info(f"ğŸ“ˆ ç»Ÿè®¡æŠ¥å‘Šï¼š{stats_file.name}")
