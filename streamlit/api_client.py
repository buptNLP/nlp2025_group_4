import requests
import streamlit as st
from typing import List, Optional, Dict, Any, Tuple
from abc import ABC, abstractmethod
from enum import Enum
import re

class APIProvider(Enum):
    """æ”¯æŒçš„ API æä¾›å•†"""
    DEEPSEEK = "deepseek"
    GLM = "glm"

class BaseAPIClient(ABC):
    """åŸºç¡€ API å®¢æˆ·ç«¯æŠ½è±¡ç±»"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key.strip() if api_key else None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥ API æ˜¯å¦å¯ç”¨"""
        return bool(self.api_key and self.api_key.strip())
    
    @abstractmethod
    def call_chat_completion(
        self, 
        prompt: str, 
        system_prompt: str = None,
        model: str = None,
        max_tokens: int = 1500,
        temperature: float = 0.7,
        timeout: int = 30
    ) -> str:
        """èŠå¤©å®Œæˆ API è°ƒç”¨çš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    def analyze_query(self, raw_query: str) -> Dict[str, Any]:
        """
        ç»¼åˆåˆ†ææŸ¥è¯¢ - ä¸€æ¬¡APIè°ƒç”¨å®Œæˆæ‰€æœ‰ä»»åŠ¡
        è¿”å›ç»“æ„åŒ–æ•°æ®ï¼š
        {
            'need_search': bool,
            'reason': str,
            'optimized_query': str,
            'bm25_weight': float,
            'vector_weight': float,
            'direct_answer': str,
            'extracted_book': str | None  # --- æ–°å¢ ---
        }
        """
        if not self.is_available():
            return {
                'need_search': True,
                'reason': 'æœªé…ç½®API',
                'optimized_query': raw_query,
                'bm25_weight': 0.3,
                'vector_weight': 0.7,
                'direct_answer': None,
                'extracted_book': None, # --- æ–°å¢ ---
            }
       
        # --- ä¿®æ”¹æç¤ºè¯ ---
        # ä¼˜åŒ–åçš„æç¤ºè¯
        prompt = f"""ä½ æ˜¯å¤å…¸æ–‡çŒ®æ£€ç´¢ä¸“å®¶ï¼Œæ“…é•¿åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾å¹¶æä¾›æœ€ä½³æ£€ç´¢ç­–ç•¥ã€‚

    ã€ç”¨æˆ·é—®é¢˜ã€‘
    {raw_query}

    ã€åˆ†æä»»åŠ¡ã€‘
    è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œç»“æ„åŒ–åˆ†æï¼š

    ## ç¬¬ä¸€æ­¥ï¼šæ£€ç´¢å¿…è¦æ€§åˆ¤æ–­
    åˆ¤æ–­æ ‡å‡†ï¼š
    âœ… éœ€è¦æ£€ç´¢çš„æƒ…å†µï¼š
    - æ¶‰åŠå¤å…¸æ–‡çŒ®å†…å®¹ï¼ˆè®ºè¯­ã€åº„å­ã€å²è®°ã€ä¸‰å›½æ¼”ä¹‰ç­‰ï¼‰
    - æŸ¥è¯¢å¤ä»£äººç‰©ã€æ€æƒ³ã€å…¸æ•…ã€åˆ¶åº¦
    - éœ€è¦å¼•ç”¨åŸæ–‡æˆ–è€ƒè¯çš„é—®é¢˜
    - å¤ä»£å†å²äº‹ä»¶æˆ–æ–‡åŒ–ç°è±¡

    âŒ æ— éœ€æ£€ç´¢çš„æƒ…å†µï¼š
    - çº¯ç°ä»£æ¦‚å¿µæˆ–æŠ€æœ¯é—®é¢˜
    - ç®€å•çš„å¸¸è¯†æ€§é—®é¢˜
    - ä¸å¤å…¸æ–‡çŒ®æ— å…³çš„å†…å®¹

    ## ç¬¬äºŒæ­¥ï¼šæŸ¥è¯¢ä¼˜åŒ–ï¼ˆä»…å½“éœ€è¦æ£€ç´¢æ—¶ï¼‰
    éµå¾ªå››å¤§åŸåˆ™ï¼š

    ### 2.1 è¯æ±‡å¤å…¸åŒ–
    - ç°ä»£è¯æ±‡ â†’ å¤ä»£å¯¹åº”è¯æ±‡
    - ç¤ºä¾‹ï¼š
    * "çš‡å¸" â†’ "å¤©å­"/"å›ä¸»"/"å¸ç‹" 
    * "å®˜å‘˜" â†’ "è‡£å·¥"/"ä»•äºº"/"æœè‡£"
    * "ç®¡ç†" â†’ "æ²»ç†"/"ç»Ÿå¾¡"/"ç»çº¬"
    * "æ•™è‚²" â†’ "æ•™åŒ–"/"è‚²æ‰"/"å¯è’™"

    ### 2.2 å¥å¼æ–‡è¨€åŒ–
    - é‡‡ç”¨å¤å…¸å¥å¼ç»“æ„

    ### 2.3 ä¿ç•™å…³é”®ä¿¡æ¯
    - é‡è¦äººåã€åœ°åã€ä¹¦åä¿æŒä¸å˜
    - å¢åŠ æ ¸å¿ƒçš„å¤å…¸è¯æè¿°ï¼Œæ‹“å±•ä¸€äº›åŒä¹‰è¯
    - æ ¸å¿ƒæ¦‚å¿µä¸èƒ½åç¦»åŸæ„
    - ä¼˜åŒ–æŸ¥è¯¢æ—¶ä¿æŒåŸæ„ä¸å˜ï¼Œåªæ”¹å˜è¡¨è¾¾æ–¹å¼
    
    - ç¤ºä¾‹ï¼š
    * "å­”å­è®¤ä¸ºä»€ä¹ˆæ˜¯ä»ï¼Ÿ" â†’ "å­”å­è®ºä»ä¹‹ä¹‰ä½•åœ¨ï¼Ÿå…‹å·±å¤ç¤¼ä¹‹è¯´ä½•è§£ï¼Ÿ"
    * "å¦‚ä½•æ²»ç†å›½å®¶ï¼Ÿ" â†’ "æ²»å›½ä¹‹é“ä½•å¦‚ï¼Ÿå¾·æ”¿ç‹é“ä¹‹æœ¯ä½•æ‹©ï¼Ÿ"
    * "å¥½å®˜å‘˜çš„æ ‡å‡†ï¼Ÿ" â†’ "è´¤è‡£ä¹‹èµ„ä½•ä»¥è¾¨ä¹‹ï¼Ÿæ¸…å»‰å¾ªåä¹‹é“ä½•åœ¨ï¼Ÿ"
    
    ### 2.4 å®Œæ•´ä¿ç•™å¤æ–‡åŸå¥
    - å¦‚æœqueryæ˜¯å¤æ–‡åŸå¥ï¼Œå®Œæ•´ä¿ç•™ï¼Œä¸éœ€è¦åšä¼˜åŒ–

    ## ç¬¬ä¸‰æ­¥ï¼šæ£€ç´¢æƒé‡ç­–ç•¥
    æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©æœ€ä½³æƒé‡ç»„åˆï¼š

    ### ğŸ¯ é«˜BM25æƒé‡ (0.6-0.8)
    **é€‚ç”¨åœºæ™¯**ï¼š
    - ç²¾ç¡®å¤æ–‡åŒ¹é…ï¼š"å­¦è€Œæ—¶ä¹ ä¹‹"
    - ä¸“æœ‰åè¯æŸ¥è¯¢ï¼š"è¯¸è‘›äº®"ã€"èµ¤å£ä¹‹æˆ˜"  
    - æœ¯è¯­å®šä¹‰ï¼š"ä»€ä¹ˆæ˜¯ä»"
    **æ¨èé…ç½®**ï¼šBM25: 0.7, Vector: 0.3

    ### ğŸ§  é«˜Vectoræƒé‡ (0.6-0.8)  
    **é€‚ç”¨åœºæ™¯**ï¼š
    - æ¦‚å¿µç†è§£ï¼š"å„’å®¶æ€æƒ³çš„æ ¸å¿ƒ"
    - ä¸»é¢˜æŸ¥è¯¢ï¼š"å¤ä»£æ•™è‚²æ–¹æ³•"
    - å“²å­¦æ¢è®¨ï¼š"åº„å­çš„äººç”Ÿè§‚"
    **æ¨èé…ç½®**ï¼šBM25: 0.3, Vector: 0.7

    ### âš–ï¸ å¹³è¡¡æƒé‡ (0.4-0.6)
    **é€‚ç”¨åœºæ™¯**ï¼š
    - å¤åˆæŸ¥è¯¢ï¼š"å­”å­å…³äºä»çš„å…·ä½“è®ºè¿°"
    - æ¯”è¾ƒåˆ†æï¼š"å­”å­å’Œè€å­æ€æƒ³å·®å¼‚"  
    - ä¸ç¡®å®šæ„å›¾çš„æŸ¥è¯¢
    **æ¨èé…ç½®**ï¼šBM25: 0.5, Vector: 0.5

    ## ç¬¬å››æ­¥ï¼šä¹¦åæå–
    ç²¾ç¡®è¯†åˆ«é—®é¢˜ä¸­æ˜ç¡®æåŠçš„å¤å…¸ä¹¦åï¼š
    - æ¶‰åŠåˆ°å¤šæœ¬ä¹¦ï¼Œè¿”å›null
    - å®Œæ•´ä¹¦åï¼šã€Šè®ºè¯­ã€‹ã€ã€Šåº„å­ã€‹ã€ã€Šå²è®°ã€‹ã€ã€Šä¸‰å›½æ¼”ä¹‰ã€‹ç­‰
    - ç®€ç§°ï¼šè®ºè¯­ã€åº„å­ã€å²è®°ã€ä¸‰å›½ç­‰
    - å¦‚æœªæ˜ç¡®æåŠä»»ä½•ä¹¦åï¼Œè¿”å›null

    ã€è¾“å‡ºè¦æ±‚ã€‘
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œåªè¾“å‡ºJSONï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰æ˜ç¡®çš„å€¼ï¼š

    ```json
    {{
        "need_search": true,
        "reason": "å…·ä½“çš„åˆ¤æ–­ç†ç”±è¯´æ˜",
        "optimized_query": "æŒ‰ä¸Šè¿°è§„åˆ™ä¼˜åŒ–åçš„æŸ¥è¯¢è¯­å¥",
        "bm25_weight": å…·ä½“çš„æƒé‡æˆ–è€…None,
        "vector_weight": å…·ä½“çš„æƒé‡æˆ–è€…None,
        "direct_answer": None,
        "extracted_book": "æ˜ç¡®çš„å•æœ¬ä¹¦å"æˆ–è€…None(è¡¨ç¤ºæ­£å¸¸æ£€ç´¢)
    }}
    ```    

    å¼€å§‹åˆ†æï¼š"""
        try:
            result = self.call_chat_completion(
                prompt=prompt,
                max_tokens=500,
                temperature=0.3,
                timeout=20
            )
            
            print(result)
            
            import json
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)
                
                # ä¿®å¤ï¼šå®‰å…¨å¤„ç†å¯èƒ½ä¸º null çš„æƒé‡å€¼
                bm25_weight = parsed.get('bm25_weight')
                vector_weight = parsed.get('vector_weight')
                
                return {
                    'need_search': bool(parsed.get('need_search', True)),
                    'reason': parsed.get('reason', ''),
                    'optimized_query': parsed.get('optimized_query', raw_query),
                    'bm25_weight': float(bm25_weight) if bm25_weight is not None else 0.3,
                    'vector_weight': float(vector_weight) if vector_weight is not None else 0.7,
                    'direct_answer': parsed.get('direct_answer', None),
                    'extracted_book': parsed.get('extracted_book')
                }
            else:
                raise ValueError("æ— æ³•è§£æJSONå“åº”")
                
        except Exception as e:
            return {
                'need_search': True,
                'reason': f'APIåˆ†æå¤±è´¥: {str(e)}',
                'optimized_query': raw_query,
                'bm25_weight': 0.3,
                'vector_weight': 0.7,
                'direct_answer': None,
                'extracted_book': None
            }
            
    
    # åœ¨ BaseAPIClient ç±»ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•
    def decompose_complex_query(self, raw_query: str) -> Dict[str, Any]:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦å¤šè½®æ£€ç´¢å¹¶æ‹†è§£å¤æ‚ä»»åŠ¡
        è¿”å›ç»“æ„ï¼š
        {
            'need_multi_round': bool,
            'reason': str,
            'subtasks': [
                {
                    'subtask_id': int,
                    'subtask_query': str,
                    'subtask_focus': str,
                    'search_weight': {'bm25': float, 'vector': float}
                }
            ],
            'synthesis_instruction': str
        }
        """
        if not self.is_available():
            return {
                'need_multi_round': False,
                'reason': 'æœªé…ç½®API',
                'subtasks': [],
                'synthesis_instruction': ''
            }
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå¤æ–‡æ£€ç´¢ä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·çš„é—®é¢˜æ˜¯å¦éœ€è¦å¤šè½®æ£€ç´¢ã€‚

    ç”¨æˆ·é—®é¢˜ï¼š{raw_query}

    åˆ†ææ­¥éª¤ï¼š
    1. åˆ¤æ–­æ˜¯å¦éœ€è¦å¤šè½®æ£€ç´¢ï¼š
    - ç®€å•çš„æ¦‚å¿µæŸ¥è¯¢ï¼ˆå¦‚"ä»€ä¹ˆæ˜¯ä»"ï¼‰ä¸éœ€è¦å¤šè½®æ£€ç´¢
    - æ¶‰åŠå¤šä¸ªæ–¹é¢çš„å¤æ‚é—®é¢˜éœ€è¦å¤šè½®æ£€ç´¢
    - éœ€è¦å¯¹æ¯”ã€åˆ†æã€ç»¼åˆçš„é—®é¢˜éœ€è¦å¤šè½®æ£€ç´¢
    - æ¶‰åŠå¤šä¸ªäººç‰©ã€æ—¶æœŸã€è§‚ç‚¹çš„é—®é¢˜éœ€è¦å¤šè½®æ£€ç´¢

    2. å¦‚æœéœ€è¦å¤šè½®æ£€ç´¢ï¼Œå°†é—®é¢˜æ‹†è§£æˆ2-5ä¸ªå­ä»»åŠ¡ï¼š
    - æ¯ä¸ªå­ä»»åŠ¡åº”è¯¥èšç„¦ä¸€ä¸ªå…·ä½“æ–¹é¢
    - å­ä»»åŠ¡ä¹‹é—´åº”è¯¥äº’è¡¥ï¼Œè¦†ç›–åŸé—®é¢˜çš„ä¸åŒç»´åº¦
    - éœ€è¦å¯¹å­ä»»åŠ¡çš„queryåšä¸€ä¸ªå¤æ–‡æ£€ç´¢çš„ä¼˜åŒ–ï¼Œé‡‡ç”¨å¤æ–‡çš„é£æ ¼å¹¶ä¿ç•™æ ¸å¿ƒå…³é”®è¯
    
    3. å¦‚æœéœ€è¦æ£€ç´¢ï¼Œåˆ†ææ£€ç´¢æƒé‡ï¼š
        3.1 é«˜BM25æƒé‡åœºæ™¯ï¼ˆ0.6-0.8ï¼‰
    - **ç²¾ç¡®å¤æ–‡åŒ¹é…**ï¼šç”¨æˆ·æä¾›å®Œæ•´å¤æ–‡å¥å­
    - **ä¸“æœ‰åè¯æŸ¥è¯¢**ï¼šç‰¹å®šäººåã€åœ°åã€å®˜èŒå
    - **å…¸ç±ä¹¦åæŸ¥è¯¢**ï¼šæ˜ç¡®æŒ‡å‘ç‰¹å®šå¤ç±
    - **æœ¯è¯­å®šä¹‰æŸ¥è¯¢**ï¼šæŸ¥è¯¢ç‰¹å®šå¤å…¸æ¦‚å¿µ
    æ¨èæƒé‡ï¼šBM25: 0.7, Vector: 0.3

        3.2 é«˜å‘é‡æƒé‡åœºæ™¯ï¼ˆ0.6-0.8ï¼‰
    - **æ¦‚å¿µæ€§ç†è§£**ï¼šæŸ¥è¯¢æ€æƒ³ã€å“²å­¦è§‚ç‚¹
    - **ä¸»é¢˜æ€§æŸ¥è¯¢**ï¼šå…³äºæŸä¸ªè¯é¢˜çš„å¹¿æ³›å†…å®¹
    - **æƒ…å¢ƒæè¿°**ï¼šç”¨ç°ä»£è¯­è¨€æè¿°å¤ä»£æƒ…å¢ƒ
    - **æ¯”è¾ƒåˆ†æ**ï¼šä¸åŒè§‚ç‚¹ã€äººç‰©çš„å¯¹æ¯”
    æ¨èæƒé‡ï¼šBM25: 0.3, Vector: 0.7

        3.3 å¹³è¡¡æƒé‡åœºæ™¯ï¼ˆ0.4-0.6ï¼‰
    - **æ··åˆæŸ¥è¯¢**ï¼šæ—¢æœ‰ç²¾ç¡®è¯æ±‡åˆæœ‰æ¦‚å¿µç†è§£
    - **ä¸ç¡®å®šæŸ¥è¯¢**ï¼šæ— æ³•æ˜ç¡®åˆ¤æ–­ç”¨æˆ·æ„å›¾
    - **å¤šå±‚æ¬¡æŸ¥è¯¢**ï¼šæ¶‰åŠå¤šä¸ªç»´åº¦çš„å¤æ‚é—®é¢˜
    æ¨èæƒé‡ï¼šBM25: 0.5, Vector: 0.5

    4. æä¾›ç»¼åˆæŒ‡å¯¼ï¼šå¦‚ä½•å°†å„å­ä»»åŠ¡çš„ç»“æœç»¼åˆæˆå®Œæ•´ç­”æ¡ˆ

    ç¤ºä¾‹æ‹†è§£ï¼š
    - "å­”å­å’Œå­Ÿå­çš„æ”¿æ²»æ€æƒ³æœ‰ä½•å¼‚åŒï¼Ÿ" â†’ 
    å­ä»»åŠ¡1: "å­”å­çš„æ”¿æ²»æ€æƒ³æ ¸å¿ƒè§‚ç‚¹"
    å­ä»»åŠ¡2: "å­Ÿå­çš„æ”¿æ²»æ€æƒ³æ ¸å¿ƒè§‚ç‚¹"
    å­ä»»åŠ¡3: "å„’å®¶æ”¿æ²»æ€æƒ³çš„æ¼”å˜"

    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
    {{
        "need_multi_round": true/false,
        "reason": "åˆ¤æ–­ç†ç”±",
        "subtasks": [
            {{
                "subtask_id": 1,
                "subtask_query": "ä¼˜åŒ–åçš„å­æŸ¥è¯¢",
                "subtask_focus": "è¯¥å­ä»»åŠ¡çš„æ ¸å¿ƒå…³æ³¨ç‚¹",
                "search_weight": {{"bm25": 0.3, "vector": 0.7}}
            }}
        ],
        "synthesis_instruction": "å¦‚ä½•ç»¼åˆå„å­ä»»åŠ¡ç»“æœçš„æŒ‡å¯¼"
    }}"""

        try:
            result = self.call_chat_completion(
                prompt=prompt,
                max_tokens=800,
                temperature=0.3,
                timeout=25
            )
            
            print(result)
            
            import json
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group(0))
                
                # å®‰å…¨å¤„ç†è¿”å›çš„æ•°æ®
                need_multi_round = bool(parsed.get('need_multi_round', False))
                
                # å¤„ç†å­ä»»åŠ¡åˆ—è¡¨
                subtasks = []
                if need_multi_round and 'subtasks' in parsed and parsed['subtasks']:
                    for subtask in parsed['subtasks']:
                        # ç¡®ä¿æ¯ä¸ªå­ä»»åŠ¡æœ‰å®Œæ•´çš„ç»“æ„
                        processed_subtask = {
                            'subtask_id': subtask.get('subtask_id', 1),
                            'subtask_query': subtask.get('subtask_query', ''),
                            'subtask_focus': subtask.get('subtask_focus', ''),
                            'search_weight': self._process_search_weight(
                                subtask.get('search_weight', {})
                            )
                        }
                        subtasks.append(processed_subtask)
                
                return {
                    'need_multi_round': need_multi_round,
                    'reason': parsed.get('reason', ''),
                    'subtasks': subtasks,
                    'synthesis_instruction': parsed.get('synthesis_instruction', '')
                }
            else:
                raise ValueError("æ— æ³•è§£æJSONå“åº”")
                
        except Exception as e:
            return {
                'need_multi_round': False,
                'reason': f'ä»»åŠ¡æ‹†è§£å¤±è´¥: {str(e)}',
                'subtasks': [],
                'synthesis_instruction': ''
            }
            
    def _process_search_weight(self, weight_dict: Dict) -> Dict[str, float]:
        """
        å®‰å…¨å¤„ç†æœç´¢æƒé‡
        """
        if not weight_dict:
            return {'bm25': 0.3, 'vector': 0.7}
        
        bm25_weight = weight_dict.get('bm25')
        vector_weight = weight_dict.get('vector')
        
        # å¤„ç† null æˆ–æ— æ•ˆå€¼
        bm25 = float(bm25_weight) if bm25_weight is not None else 0.3
        vector = float(vector_weight) if vector_weight is not None else 0.7
        
        # ç¡®ä¿æƒé‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        bm25 = max(0.0, min(1.0, bm25))
        vector = max(0.0, min(1.0, vector))
        
        # å½’ä¸€åŒ–æƒé‡
        total = bm25 + vector
        if total > 0:
            bm25 = bm25 / total
            vector = vector / total
        else:
            bm25, vector = 0.3, 0.7
        
        return {'bm25': bm25, 'vector': vector}

    def synthesize_multi_round_results(self, query: str, subtasks_results: List[Dict]) -> str:
        """
        ç»¼åˆå¤šè½®æ£€ç´¢çš„ç»“æœ
        subtasks_results: æ¯ä¸ªå­ä»»åŠ¡çš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self.is_available():
            return self._basic_synthesis(query, subtasks_results)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for subtask_result in subtasks_results:
            subtask_query = subtask_result['subtask_query']
            subtask_focus = subtask_result['subtask_focus']
            results = subtask_result['results']
            
            context_parts.append(f"\n### å­ä»»åŠ¡ï¼š{subtask_query}")
            context_parts.append(f"å…³æ³¨ç‚¹ï¼š{subtask_focus}")
            
            for i, item in enumerate(results[:2], 1):  # æ¯ä¸ªå­ä»»åŠ¡å–å‰2ä¸ªç»“æœ
                meta = item['metadata']
                context_parts.append(
                    f"{i}. ã€Š{meta['book']}Â·{meta['chapter']}ã€‹: {item['content']}"
                )
        
        context_text = "\n".join(context_parts)
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­å›½å¤å…¸æ–‡å­¦çš„å­¦è€…ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªå¤æ‚é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å¤šè½®æ£€ç´¢è·å¾—äº†ç›¸å…³å¤æ–‡ã€‚

    ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š{query}

    å¤šè½®æ£€ç´¢ç»“æœï¼š
    {context_text}

    ç»¼åˆæŒ‡å¯¼ï¼š{subtasks_results[0].get('synthesis_instruction', 'è¯·ç»¼åˆæ‰€æœ‰ç›¸å…³å†…å®¹ï¼Œç»™å‡ºå…¨é¢çš„å›ç­”')}

    è¯·åŸºäºä»¥ä¸Šå¤šè½®æ£€ç´¢çš„ç»“æœï¼Œç»¼åˆå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¦æ±‚ï¼š
    1. æ•´åˆå„å­ä»»åŠ¡çš„å‘ç°ï¼Œå½¢æˆè¿è´¯çš„å›ç­”
    2. çªå‡ºé‡ç‚¹ï¼Œæ¡ç†æ¸…æ™°
    3. å¼•ç”¨å…·ä½“åŸæ–‡æ”¯æ’‘è§‚ç‚¹
    4. æä¾›æ·±å…¥çš„åˆ†æå’Œè§è§£
    5. å¦‚æœæ¶‰åŠå¯¹æ¯”ï¼Œè¦æ˜ç¡®æŒ‡å‡ºå¼‚åŒ

    è¯·ç»™å‡ºç»¼åˆã€æ·±å…¥ã€æœ‰è§åœ°çš„å›ç­”ï¼š"""

        return self.call_chat_completion(
            prompt=prompt,
            system_prompt="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¤å…¸æ–‡å­¦å­¦è€…ï¼Œæ“…é•¿ç»¼åˆåˆ†æå’Œæ·±åº¦è§£è¯»ã€‚",
            max_tokens=2000,
            temperature=0.7
        )

    def _basic_synthesis(self, query: str, subtasks_results: List[Dict]) -> str:
        """åŸºç¡€ç»¼åˆæ–¹æ³•ï¼ˆæ— APIæ—¶ä½¿ç”¨ï¼‰"""
        synthesis = f"å…³äºã€Œ{query}ã€çš„å¤šè§’åº¦åˆ†æï¼š\n\n"
        
        for idx, subtask_result in enumerate(subtasks_results, 1):
            synthesis += f"**{idx}. {subtask_result['subtask_focus']}**\n"
            synthesis += f"æ£€ç´¢ï¼š{subtask_result['subtask_query']}\n"
            
            for item in subtask_result['results'][:2]:
                meta = item['metadata']
                synthesis += f"- ã€Š{meta['book']}Â·{meta['chapter']}ã€‹ï¼š{item['content']}\n"
            
            synthesis += "\n"
        
        return synthesis    
    
    def generate_answer(self, query: str, context: list) -> str:
        """ç”Ÿæˆç­”æ¡ˆ - é€šç”¨å®ç°"""
        if not self.is_available():
            return self._generate_basic_answer(query, context)
        
        context_parts = []
        for i,item in enumerate(context[:3]):
            meta = item['metadata']
            context_parts.append(
                f"ã€{meta['book']} Â· {meta['chapter']}ã€‘: {item['content']}"
            )
            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥
            nested_meta = meta.get('metadata', {})
            if 'prev_context' in nested_meta and nested_meta['prev_context']:
                context_parts[i] += f"\nå‰æ–‡ï¼š{nested_meta['prev_context']}"
            if 'next_context' in nested_meta and nested_meta['next_context']:
                context_parts[i] += f"\nåæ–‡ï¼š{nested_meta['next_context']}"

        context_text = "\n\n".join(context_parts)
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­å›½å¤å…¸æ–‡å­¦çš„å­¦è€…ï¼Œè¯·åŸºäºä»¥ä¸‹å¤æ–‡åŸæ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç›¸å…³å¤æ–‡åŸæ–‡åŠä¸Šä¸‹æ–‡ï¼š
{context_text}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚åˆ†ç‚¹å›ç­”ï¼š
1. é¦–å…ˆå¼•ç”¨æœ€ç›¸å…³çš„åŸæ–‡
2. è¯¦ç»†å®Œæ•´åœ°å›ç­”ç”¨æˆ·é—®é¢˜
3. è§£é‡Šå¤æ–‡çš„å­—é¢å«ä¹‰
4. é˜è¿°å…¶æ·±å±‚æ€æƒ³å†…æ¶µ
5. ç»“åˆç°ä»£è§‚ç‚¹è¿›è¡Œåˆ†æ
6. æä¾›å®é™…çš„æŒ‡å¯¼æ„ä¹‰

å›ç­”è¦æ±‚ï¼šç¬¬äºŒéƒ¨åˆ†è¯¦ç»†å›ç­”ï¼Œå…¶ä½™å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡ºï¼Œæ—¢æœ‰å­¦æœ¯æ·±åº¦åˆé€šä¿—æ˜“æ‡‚ã€‚

å›ç­”ï¼š"""
        return self.call_chat_completion(
            prompt=prompt,
            system_prompt="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¤å…¸æ–‡å­¦å­¦è€…ã€‚"
        )
    
    def _generate_basic_answer(self, query: str, context: list) -> str:
        """ç”ŸæˆåŸºç¡€å›ç­”ï¼ˆæ— APIæ—¶ä½¿ç”¨ï¼‰"""
        answer = f"å…³äºã€Œ{query}ã€ï¼Œåœ¨å¤æ–‡ä¸­æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n\n"
        
        for i, item in enumerate(context[:3], 1):
            meta = item['metadata']
            answer += f"**{i}. ã€Š{meta['book']}Â·{meta['chapter']}ã€‹**\n"
            answer += f"åŸæ–‡ï¼šã€Œ{item['content']}ã€\n"
            answer += f"è¯é¢˜ï¼š{meta['topic']}\n\n"
        
        return answer


class DeepSeekAPIClient(BaseAPIClient):
    """DeepSeek API å®¢æˆ·ç«¯å®ç°"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com"):
        super().__init__(api_key)
        self.base_url = base_url
        self.default_model = "deepseek-chat"
    
    def call_chat_completion(
        self, 
        prompt: str, 
        system_prompt: str = None,
        model: str = None,
        max_tokens: int = 1500,
        temperature: float = 0.7,
        timeout: int = 30
    ) -> str:
        if not self.is_available():
            return "APIå¯†é’¥æœªé…ç½®"
        
        try:
            messages = []
            if system_prompt:
                messages.append({'role': 'system', 'content': system_prompt})
            messages.append({'role': 'user', 'content': prompt})
            
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {self.api_key}'
            }
            
            data = {
                'model': model or self.default_model,
                'messages': messages,
                'stream': False,
                'max_tokens': max_tokens,
                'temperature': temperature
            }
            
            response = requests.post(
                f'{self.base_url}/chat/completions',
                headers=headers,
                json=data,
                timeout=timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content'].strip()
            else:
                return f"APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}"
                
        except requests.exceptions.Timeout:
            return "APIè°ƒç”¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        except requests.exceptions.RequestException as e:
            return f"ç½‘ç»œé”™è¯¯: {str(e)}"
        except Exception as e:
            return f"APIè°ƒç”¨å‡ºé”™: {str(e)}"


class GLMAPIClient(BaseAPIClient):
    """æ™ºè°± GLM API å®¢æˆ·ç«¯å®ç°"""
    
    def __init__(self, api_key: str):
        super().__init__(api_key)
        self.default_model = "glm-4-air-250414"
        self._client = None
    
    @property
    def client(self):
        """å»¶è¿Ÿåˆå§‹åŒ– ZhipuAI å®¢æˆ·ç«¯"""
        if self._client is None and self.api_key:
            try:
                from zhipuai import ZhipuAI
                self._client = ZhipuAI(api_key=self.api_key)
            except ImportError:
                raise ImportError("è¯·å…ˆå®‰è£… zhipuai åº“: pip install zhipuai")
        return self._client
    
    def call_chat_completion(
        self, 
        prompt: str, 
        system_prompt: str = None,
        model: str = None,
        max_tokens: int = 1500,
        temperature: float = 0.7,
        timeout: int = 30
    ) -> str:
        if not self.is_available():
            return "APIå¯†é’¥æœªé…ç½®"
        
        try:
            messages = []
            if system_prompt:
                messages.append({'role': 'system', 'content': system_prompt})
            messages.append({'role': 'user', 'content': prompt})
            
            # è°ƒç”¨ GLM API
            response = self.client.chat.completions.create(
                model=model or self.default_model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=timeout
            )
            
            # æå–å“åº”å†…å®¹
            if hasattr(response, 'choices') and len(response.choices) > 0:
                return response.choices[0].message.content.strip()
            else:
                return "GLM API è¿”å›æ ¼å¼å¼‚å¸¸"
                
        except ImportError as e:
            return f"GLM åº“æœªå®‰è£…: {str(e)}"
        except Exception as e:
            return f"GLM API è°ƒç”¨å‡ºé”™: {str(e)}"


class APIClientFactory:
    """API å®¢æˆ·ç«¯å·¥å‚ç±»"""
    
    @staticmethod
    def create_client(
        provider: APIProvider,
        api_key: str,
        **kwargs
    ) -> BaseAPIClient:
        """
        åˆ›å»º API å®¢æˆ·ç«¯å®ä¾‹
        
        Args:
            provider: API æä¾›å•†
            api_key: API å¯†é’¥
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚ base_urlï¼‰
            
        Returns:
            BaseAPIClient å®ä¾‹
        """
        if provider == APIProvider.DEEPSEEK:
            return DeepSeekAPIClient(
                api_key=api_key,
                base_url=kwargs.get('base_url', 'https://api.deepseek.com')
            )
        elif provider == APIProvider.GLM:
            return GLMAPIClient(api_key=api_key)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ API æä¾›å•†: {provider}")


# ä½¿ç”¨ç¤ºä¾‹
def get_api_client(config: Dict[str, Any]) -> BaseAPIClient:
    """
    ä»é…ç½®ä¸­è·å– API å®¢æˆ·ç«¯
    
    é…ç½®ç¤ºä¾‹:
    config = {
        'provider': 'deepseek',  # æˆ– 'glm'
        'api_key': 'your-api-key',
        'base_url': 'https://api.deepseek.com'  # å¯é€‰ï¼Œä»… DeepSeek éœ€è¦
    }
    """
    provider = APIProvider(config.get('provider', 'deepseek'))
    api_key = config.get('api_key', '')
    
    return APIClientFactory.create_client(
        provider=provider,
        api_key=api_key,
        base_url=config.get('base_url')
    )
