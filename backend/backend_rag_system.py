import chromadb
from chromadb.utils import embedding_functions
import requests
import json
import re
import csv
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import asdict
import os

# å¯¼å…¥åç«¯å·¥å…·
from backend_utils import backend_logger as st
from backend_config import HAS_BM25, HAS_RERANKER
from models import AncientTextSegment
from text_processing import SmartTextChunker, AncientTextAnalyzer
from backend_retrieval import HybridRetriever  # ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬
from api_client import get_api_client, APIProvider

if HAS_RERANKER:
    import torch

class UniversalAncientRAG:
    """é€šç”¨å¤æ–‡RAGç³»ç»Ÿ - ä¿®å¤ç‰ˆæœ¬ï¼Œè§£å†³é‡æ’åºé‡å¤åŠ è½½é—®é¢˜"""
    
    # def __init__(self, embedding_model: str = "BAAI/bge-large-zh-v1.5",
    def __init__(self, embedding_model: str = "BAAI/bge-large-zh-v1.5",
             max_chunk_size: int = 150,
             min_chunk_size: int = 20,
             context_window: int = 80,
             use_reranker: bool = None,
             reranker_model: str = None):
        self.client = chromadb.Client()
        self.collection_name = "ancient_texts_collection"
        self.embedding_model = embedding_model
        self.segments: List[AncientTextSegment] = []
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®é…ç½®é‡æ’åºé€‰é¡¹
        self.use_reranker = use_reranker if use_reranker is not None else (
            HAS_RERANKER and os.getenv("RAG_USE_RERANKER", "true").lower() == "true"
        )
        self.reranker_model = reranker_model or os.getenv("RAG_RERANKER_MODEL", "BAAI/bge-reranker-large")
        
        # ä½¿ç”¨é…ç½®å‚æ•°åˆå§‹åŒ–åˆ†å—å™¨
        self.chunker = SmartTextChunker(
            max_chunk_size=max_chunk_size,
            min_chunk_size=min_chunk_size,
            context_window=context_window
        )
        
        self.analyzer = AncientTextAnalyzer()
        self.retriever: Optional[HybridRetriever] = None
        self.api_client = None
            
        # é…ç½®embeddingå‡½æ•°
        self._setup_embedding_function()
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._initialize_collection()

    def set_api_config(self, config: Dict[str, Any]):
        """è®¾ç½®APIé…ç½®å¹¶åˆ›å»ºç›¸åº”çš„å®¢æˆ·ç«¯"""
        try:
            # ä½¿ç”¨å·¥å‚æ–¹æ³•åˆ›å»º API å®¢æˆ·ç«¯
            self.api_client = get_api_client(config)
    
            # å­˜å‚¨å½“å‰é…ç½®ï¼ˆç”¨äºæ˜¾ç¤ºçŠ¶æ€ï¼‰
            self.current_api_config = config
            st.info(f"âœ… å·²é…ç½® {config.get('provider', 'unknown').upper()} API")
            
        except Exception as e:
            st.error(f"è®¾ç½® API å®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            self.api_client = None

    def update_chunker_params(self, max_chunk_size: int = None, 
                         min_chunk_size: int = None, 
                         context_window: int = None):
        """æ›´æ–°åˆ†å—å™¨å‚æ•°"""
        if max_chunk_size is not None:
            self.chunker.max_chunk_size = max_chunk_size
        if min_chunk_size is not None:
            self.chunker.min_chunk_size = min_chunk_size
        if context_window is not None:
            self.chunker.context_window = context_window
        
        # è¿”å›å½“å‰å‚æ•°ï¼Œç”¨äºæ˜¾ç¤º
        return {
            'max_chunk_size': self.chunker.max_chunk_size,
            'min_chunk_size': self.chunker.min_chunk_size,
            'context_window': self.chunker.context_window
        }
    
    def update_reranker_config(self, use_reranker: bool = None, reranker_model: str = None):
        """æ›´æ–°é‡æ’åºé…ç½®"""
        if use_reranker is not None:
            self.use_reranker = use_reranker and HAS_RERANKER
        
        if reranker_model is not None:
            self.reranker_model = reranker_model
        
        # å¦‚æœæ£€ç´¢å™¨å·²åˆå§‹åŒ–ï¼Œæ›´æ–°å…¶é…ç½®
        if self.retriever:
            self.retriever.update_config(
                use_reranker=self.use_reranker,
                reranker_model=self.reranker_model
            )
        
        st.info(f"ğŸ”„ é‡æ’åºé…ç½®å·²æ›´æ–°: å¯ç”¨={self.use_reranker}, æ¨¡å‹={self.reranker_model}")
    
    def _setup_embedding_function(self):
        """å¯ç”¨GPUåŠ é€Ÿçš„embedding"""
        try:
            if HAS_RERANKER:
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                st.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
                
                self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                    model_name=self.embedding_model,
                    device=device
                )
            else:
                self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                    model_name=self.embedding_model
                )
        except Exception as e:
            st.warning(f"Embedding å‡½æ•°åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name=self.embedding_model
            )
    
    def _initialize_collection(self):
        """åˆå§‹åŒ–collection"""
        try:
            # å°è¯•è·å–ç°æœ‰collection
            self.collection = self.client.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function
            )
        except:
            # åˆ›å»ºæ–°collection
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"description": f"å¤æ–‡å‘é‡æ•°æ®åº“ - {self.embedding_model}"}
            )

    def change_embedding_model(self, new_model: str):
        """æ›´æ¢embeddingæ¨¡å‹å¹¶é‡å»ºç´¢å¼•"""
        st.info(f"ğŸ”„ åˆ‡æ¢embeddingæ¨¡å‹: {self.embedding_model} â†’ {new_model}")
        
        # ä¿å­˜å½“å‰æ•°æ®
        old_segments = self.segments.copy()
        
        # åˆ é™¤æ—§collection
        try:
            self.client.delete_collection(self.collection_name)
        except:
            pass
        
        # æ›´æ–°æ¨¡å‹
        self.embedding_model = new_model
        self._setup_embedding_function()
        self._initialize_collection()
        
        # é‡å»ºç´¢å¼•
        if old_segments:
            self.segments = old_segments
            self._build_vector_database()
            st.success(f"âœ… å·²åˆ‡æ¢åˆ° {new_model} å¹¶é‡å»ºç´¢å¼•")

    def load_from_directory(self, root_dir: str, file_extensions: List[str] = None) -> int:
        """é€’å½’åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰å¤æ–‡æ•°æ®"""
        if file_extensions is None:
            file_extensions = ['.txt', '.md', '.text']
        
        root_path = Path(root_dir)
        total_segments = 0
        
        if not root_path.exists():
            st.error(f"ç›®å½•ä¸å­˜åœ¨: {root_dir}")
            return 0
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰æŒ‡å®šæ ¼å¼çš„æ–‡ä»¶
        st.info("æ­£åœ¨æ‰«ææ–‡ä»¶...")
        
        all_text_files = []
        for ext in file_extensions:
            pattern = f"**/*{ext}"
            files = list(root_path.glob(pattern))
            all_text_files.extend(files)
        
        # å»é‡å¹¶æ’åº
        all_text_files = sorted(list(set(all_text_files)))
        
        if not all_text_files:
            st.warning(f"åœ¨æŒ‡å®šç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(file_extensions)}")
            return 0
        
        st.info(f"å‘ç° {len(all_text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        
        # åˆ›å»ºå¤„ç†ç»Ÿè®¡
        processing_stats = {
            'processed_files': 0,
            'empty_files': 0,
            'error_files': 0,
            'total_segments': 0
        }
        
        # å¤„ç†æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶
        for file_idx, text_file in enumerate(all_text_files):
            if file_idx % 10 == 0:  # æ¯10ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                st.progress(
                    file_idx / len(all_text_files), 
                    f"æ­£åœ¨å¤„ç†: {text_file.name} ({file_idx + 1}/{len(all_text_files)})"
                )
            
            try:
                # è§£ææ–‡ä»¶è·¯å¾„ä»¥è·å–ä¹¦åå’Œç¯‡ç« ä¿¡æ¯
                book_name, chapter_name = self._parse_file_path(text_file, root_path)
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                content = self._read_text_file(text_file)
                
                if content and len(content.strip()) > 10:  # å¿½ç•¥è¿‡çŸ­çš„æ–‡ä»¶
                    segments = self._process_chapter(book_name, chapter_name, content, str(text_file))
                    self.segments.extend(segments)
                    total_segments += len(segments)
                    processing_stats['total_segments'] += len(segments)
                    processing_stats['processed_files'] += 1
                    
                    # è®°å½•å¤„ç†ç»“æœ
                    if len(segments) > 0:
                        st.info(f"âœ… {book_name}/{chapter_name}: {len(segments)} æ®µ")
                else:
                    processing_stats['empty_files'] += 1
                    st.warning(f"âš ï¸ ç©ºæ–‡ä»¶: {text_file.name}")
                    
            except Exception as e:
                processing_stats['error_files'] += 1
                st.error(f"âŒ å¤„ç†å¤±è´¥: {text_file.name} - {str(e)}")
        
        # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
        st.info("ğŸ“Š å¤„ç†ç»Ÿè®¡")
        st.metric("æˆåŠŸå¤„ç†", processing_stats['processed_files'])
        st.metric("ç©ºæ–‡ä»¶", processing_stats['empty_files'])
        st.metric("é”™è¯¯æ–‡ä»¶", processing_stats['error_files'])
        st.metric("æ€»æ–‡æœ¬æ®µ", processing_stats['total_segments'])
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        if self.segments:
            with st.spinner("æ„å»ºå‘é‡æ•°æ®åº“..."):
                self._build_vector_database()
            st.success(f"åŠ è½½å®Œæˆï¼å…±å¤„ç† {total_segments} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        else:
            st.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®")
        
        return total_segments
    
    def load_single_directory(self, root_dir: str, file_extensions: List[str] = None) -> int:
        """åŠ è½½å•ä¸ªç›®å½•ï¼ˆéé€’å½’ï¼‰çš„å¤æ–‡æ•°æ®"""
        if file_extensions is None:
            file_extensions = ['.txt', '.md', '.text']
        
        root_path = Path(root_dir)
        total_segments = 0
        
        if not root_path.exists():
            st.error(f"ç›®å½•ä¸å­˜åœ¨: {root_dir}")
            return 0
        
        # æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶
        st.info("æ­£åœ¨æ‰«ææ–‡ä»¶...")
        
        all_text_files = []
        for ext in file_extensions:
            files = list(root_path.glob(f"*{ext}"))
            all_text_files.extend(files)
        
        # ä¹Ÿæ£€æŸ¥ç›´æ¥å­ç›®å½•ä¸­çš„æ–‡ä»¶
        for subdir in root_path.iterdir():
            if subdir.is_dir():
                for ext in file_extensions:
                    files = list(subdir.glob(f"*{ext}"))
                    all_text_files.extend(files)
        
        all_text_files = sorted(list(set(all_text_files)))
        
        if not all_text_files:
            st.warning(f"åœ¨æŒ‡å®šç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(file_extensions)}")
            return 0
        
        st.info(f"å‘ç° {len(all_text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        
        # å¤„ç†æ–‡ä»¶çš„é€»è¾‘ä¸é€’å½’ç‰ˆæœ¬ç›¸åŒ
        for file_idx, text_file in enumerate(all_text_files):
            if file_idx % 10 == 0:
                st.progress(
                    file_idx / len(all_text_files), 
                    f"æ­£åœ¨å¤„ç†: {text_file.name} ({file_idx + 1}/{len(all_text_files)})"
                )
            
            try:
                book_name, chapter_name = self._parse_file_path(text_file, root_path)
                content = self._read_text_file(text_file)
                
                if content and len(content.strip()) > 10:
                    segments = self._process_chapter(book_name, chapter_name, content, str(text_file))
                    self.segments.extend(segments)
                    total_segments += len(segments)
                    
                    if len(segments) > 0:
                        st.success(f"âœ… {book_name}/{chapter_name}: {len(segments)} æ®µ")
                        
            except Exception as e:
                st.error(f"âŒ å¤„ç†å¤±è´¥: {text_file.name} - {str(e)}")
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        if self.segments:
            with st.spinner("æ„å»ºå‘é‡æ•°æ®åº“..."):
                self._build_vector_database()
        
        st.success(f"åŠ è½½å®Œæˆï¼å…±å¤„ç† {total_segments} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        return total_segments
    
    def _parse_file_path(self, file_path: Path, root_path: Path) -> Tuple[str, str]:
        """æ™ºèƒ½è§£ææ–‡ä»¶è·¯å¾„ä»¥è·å–ä¹¦åå’Œç¯‡ç« ä¿¡æ¯"""
        try:
            # è·å–ç›¸å¯¹äºæ ¹ç›®å½•çš„è·¯å¾„
            relative_path = file_path.relative_to(root_path)
            path_parts = list(relative_path.parts[:-1])  # æ’é™¤æ–‡ä»¶å
            
            # æ ¹æ®ç›®å½•å±‚æ¬¡æ™ºèƒ½è§£æ
            if len(path_parts) == 0:
                # ç›´æ¥åœ¨æ ¹ç›®å½•ï¼štext.txt
                book_name = root_path.name
                chapter_name = file_path.stem
            elif len(path_parts) == 1:
                # ä¸€å±‚ç›®å½•ï¼šä¹¦å/text.txt æˆ– ç¯‡ç« /text.txt
                book_name = path_parts[0]
                chapter_name = file_path.stem if file_path.stem != 'text' else path_parts[0]
            elif len(path_parts) == 2:
                # æ ‡å‡†ç»“æ„ï¼šä¹¦å/ç¯‡ç« /text.txt
                book_name = path_parts[0]
                chapter_name = path_parts[1]
            else:
                # æ·±å±‚ç»“æ„ï¼šä¹¦å/å·/ç¯‡ç« /å­ç« èŠ‚/text.txt
                book_name = path_parts[0]
                # å°†ä¸­é—´å±‚çº§ç”¨å±‚æ¬¡åˆ†éš”ç¬¦è¿æ¥
                chapter_parts = path_parts[1:]
                chapter_name = " > ".join(chapter_parts)
            
            # æ¸…ç†åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            book_name = self._clean_name(book_name)
            chapter_name = self._clean_name(chapter_name)
            
            return book_name, chapter_name
            
        except Exception as e:
            # å‡ºé”™æ—¶ä½¿ç”¨é»˜è®¤å€¼
            return root_path.name, file_path.stem
    
    def _clean_name(self, name: str) -> str:
        """æ¸…ç†æ–‡ä»¶/ç›®å½•åç§°"""
        # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€
        name = re.sub(r'^\d+[-._]\s*', '', name)
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        name = re.sub(r'[_-]+', ' ', name)
        return name.strip()
    
    def _process_chapter(self, book: str, chapter: str, content: str, file_path: str = "") -> List[AncientTextSegment]:
        """å¤„ç†å•ä¸ªç¯‡ç« ï¼ˆä½¿ç”¨æ–°çš„å±‚çº§åˆ†å—ï¼‰"""
        segments = []
        
        # ä½¿ç”¨æ–°çš„åˆ†å—æ–¹æ³•
        chunks = self.chunker.chunk_text(content)
        
        for chunk in chunks:
            # åˆ†ææ–‡æœ¬
            topic = self.analyzer.classify_topic(chunk.content)
            
            # åˆ›å»ºç‰‡æ®µID
            segment_id = f"{book}_{chapter}_{chunk.paragraph_index:03d}_{chunk.sub_index:02d}"
            
            # æ•´åˆå…ƒæ•°æ®
            metadata = {
                'length': len(chunk.content),
                'paragraph_index': chunk.paragraph_index,
                'sub_index': chunk.sub_index,
                'is_continuation': chunk.is_continuation,
                'prev_context': chunk.prev_context,
                'next_context': chunk.next_context,
                'classical_terms': self._extract_classical_terms(chunk.content),
            }
            
            # å¦‚æœchunkæœ‰é¢å¤–çš„metadataï¼Œåˆå¹¶è¿›æ¥
            if chunk.metadata:
                metadata.update(chunk.metadata)
            
            # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            full_context = f"{chunk.prev_context} **{chunk.content}** {chunk.next_context}"
            
            segment = AncientTextSegment(
                book=book,
                chapter=chapter,
                content=chunk.content,
                topic=topic,
                segment_id=segment_id,
                context=full_context,
                metadata=metadata
            )
            segments.append(segment)
        
        return segments
    
    def _extract_classical_terms(self, text: str) -> List[str]:
        """æå–å¤å…¸æœ¯è¯­"""
        classical_terms = [
            'ä»', 'ä¹‰', 'ç¤¼', 'æ™º', 'ä¿¡', 'å¾·', 'é“', 'å¤©', 'å›å­', 'å°äºº',
            'å­¦', 'ä¹ ', 'æ•™', 'è¯²', 'æ”¿', 'æ²»', 'æ°‘', 'å›½', 'å®¶', 'å­',
            'æ‚Œ', 'å¿ ', 'æ•', 'è¯š', 'æ­£', 'ä¿®', 'é½', 'æ²»', 'å¹³'
        ]
        
        found_terms = []
        for term in classical_terms:
            if term in text:
                found_terms.append(term)
        
        return found_terms
    
    def _read_text_file(self, file_path: Path) -> str:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # å¤„ç†å¯èƒ½çš„XMLæ ‡ç­¾
            content_match = re.search(r'<content>(.*?)</content>', content, re.DOTALL)
            if content_match:
                return content_match.group(1).strip()
            
            return content
        except Exception as e:
            st.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def _build_vector_database(self):
        """æ„å»ºå‘é‡æ•°æ®åº“ - ä¿®å¤ç‰ˆæœ¬ï¼Œæ­£ç¡®ä¼ é€’é‡æ’åºé…ç½®"""
        # æ¸…ç©ºæ—§æ•°æ®
        try:
            if self.collection.count() > 0:
                all_data = self.collection.get()
                if all_data['ids']:
                    self.collection.delete(ids=all_data['ids'])
        except Exception as e:
            st.warning(f"æ¸…ç©ºæ•°æ®åº“æ—¶å‡ºç°é—®é¢˜ï¼Œå°†é‡æ–°åˆ›å»ºï¼š{e}")
            try:
                self.client.delete_collection(self.collection_name)
            except:
                pass
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "é€šç”¨å¤æ–‡å‘é‡æ•°æ®åº“"}
            )
        
        # æ‰¹é‡æ·»åŠ 
        batch_size = 5000
        for i in range(0, len(self.segments), batch_size):
            batch_segments = self.segments[i:i+batch_size]
            
            documents = [seg.content for seg in batch_segments]
            
            # è¿‡æ»¤å¹¶è½¬æ¢å…ƒæ•°æ®ï¼Œç¡®ä¿åªåŒ…å«åŸºæœ¬æ•°æ®ç±»å‹
            metadatas = []
            for seg in batch_segments:
                # åˆ›å»ºåŸºç¡€å…ƒæ•°æ®
                base_metadata = {
                    'book': seg.book,
                    'chapter': seg.chapter,
                    'topic': seg.topic,
                    'segment_id': seg.segment_id
                }
                
                # å¤„ç†æ‰©å±•å…ƒæ•°æ®
                if hasattr(seg, 'metadata') and seg.metadata:
                    filtered_extended = self._filter_metadata_for_chromadb(seg.metadata)
                    base_metadata.update(filtered_extended)
                
                metadatas.append(base_metadata)
            
            ids = [seg.segment_id for seg in batch_segments]
            
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )

        if self.segments:
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®ä¼ é€’é‡æ’åºé…ç½®
            st.info(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ£€ç´¢å™¨")
            st.info(f"   é‡æ’åº: {'å¯ç”¨' if self.use_reranker else 'ç¦ç”¨'}")
            st.info(f"   æ¨¡å‹: {self.reranker_model}")
            
            self.retriever = HybridRetriever(
                self.collection, 
                self.segments,
                use_reranker=self.use_reranker,      # ğŸ”§ ä¿®å¤ï¼šä¼ é€’é…ç½®
                reranker_model=self.reranker_model   # ğŸ”§ ä¿®å¤ï¼šä¼ é€’é…ç½®
            )

            st.success("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
            
    def _filter_metadata_for_chromadb(self, metadata: Dict) -> Dict:
        """è¿‡æ»¤å…ƒæ•°æ®ï¼Œç¡®ä¿åªåŒ…å«ChromaDBæ”¯æŒçš„æ•°æ®ç±»å‹"""
        filtered = {}
        
        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool)):
                # åŸºæœ¬ç±»å‹ç›´æ¥ä¿ç•™
                filtered[key] = value
            elif isinstance(value, list):
                # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²å’Œè®¡æ•°
                if value:  # éç©ºåˆ—è¡¨
                    filtered[f"{key}_str"] = ', '.join(map(str, value))
                    filtered[f"{key}_count"] = len(value)
                else:
                    filtered[f"{key}_count"] = 0
            elif isinstance(value, dict):
                # å°†å­—å…¸è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                filtered[f"{key}_str"] = str(value)
            else:
                # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                filtered[f"{key}_str"] = str(value)
        
        return filtered

    def search(self, query: str, top_k: int = 5, 
            search_mode: str = 'hybrid',
            metadata_filter: Dict[str, str] = None,
            bm25_weight: float = None,
            vector_weight: float = None) -> List[Dict[str, Any]]:
        """
        ç»Ÿä¸€çš„æœç´¢æ¥å£
        """
        if not self.retriever:
            st.error("æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåŠ è½½æ•°æ®")
            return []
        
        try:
            # å®šä¹‰æœç´¢æ–¹æ³•æ˜ å°„
            search_methods = {
                'hybrid': {
                    'with_filter': self.retriever.hybrid_search_with_filter,
                    'without_filter': self.retriever.hybrid_search,
                    'fallback': self.retriever._vector_search
                },
                'vector': {
                    'with_filter': self.retriever.vector_search_with_filter,
                    'without_filter': self.retriever._vector_search,
                    'fallback': None
                },
                'bm25': {
                    'with_filter': self.retriever.bm25_search_with_filter,
                    'without_filter': self.retriever._bm25_search,
                    'fallback': None
                }
            }
            
            # è·å–å¯¹åº”çš„æœç´¢æ–¹æ³•
            mode_methods = search_methods.get(search_mode)
            if not mode_methods:
                st.error(f"ä¸æ”¯æŒçš„æœç´¢æ¨¡å¼: {search_mode}")
                return []
            
            # ç‰¹æ®Šå¤„ç†ï¼šBM25ä¸å¯ç”¨æ—¶çš„é™çº§
            if search_mode == 'bm25' and not HAS_BM25:
                st.warning("BM25æ£€ç´¢ä¸å¯ç”¨ï¼Œè¯·å®‰è£… rank-bm25 åŒ…")
                return []
            
            if search_mode == 'hybrid' and not HAS_BM25:
                st.info("BM25ä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å‘é‡æ£€ç´¢æ¨¡å¼")
                # ä½¿ç”¨é™çº§æ–¹æ³•
                if metadata_filter:
                    return self.retriever.vector_search_with_filter(
                        query, top_k, metadata_filter
                    )
                else:
                    return mode_methods['fallback'](query, top_k)
            
            # æ‰§è¡Œæœç´¢
            if metadata_filter:
                # æœ‰è¿‡æ»¤æ¡ä»¶
                return mode_methods['with_filter'](
                    query, top_k, metadata_filter,
                    bm25_weight=bm25_weight,
                    vector_weight=vector_weight
                )
            else:
                # æ— è¿‡æ»¤æ¡ä»¶
                method = mode_methods['without_filter']
                if search_mode == 'hybrid':
                    # hybridæ¨¡å¼éœ€è¦ä¼ é€’æƒé‡å‚æ•°
                    return method(
                        query, top_k,
                        bm25_weight=bm25_weight,
                        vector_weight=vector_weight
                    )
                else:
                    # å…¶ä»–æ¨¡å¼ä¸éœ€è¦æƒé‡å‚æ•°
                    return method(query, top_k)
                
        except Exception as e:
            st.error(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return []

    def search_with_metadata(self, query: str, top_k: int = 5, 
                            search_mode: str = 'hybrid', 
                            metadata_filter: Dict[str, str] = None,
                            bm25_weight: float = None,
                            vector_weight: float = None) -> List[Dict[str, Any]]:
        """å‘åå…¼å®¹çš„æ–¹æ³•ï¼Œè°ƒç”¨ç»Ÿä¸€çš„searchæ–¹æ³•"""
        return self.search(
            query=query,
            top_k=top_k,
            search_mode=search_mode,
            metadata_filter=metadata_filter,
            bm25_weight=bm25_weight,
            vector_weight=vector_weight
        )
    
    def analyze_and_search(self, query: str, top_k: int = 5, 
                  search_mode: str = 'hybrid',
                  metadata_filter: Dict[str, str] = None) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
        """
        ç»¼åˆåˆ†æå¹¶æ‰§è¡Œæœç´¢
        è¿”å›ï¼š(åˆ†æç»“æœ, æœç´¢ç»“æœ)
        """
        # å¦‚æœæ²¡æœ‰APIå®¢æˆ·ç«¯ï¼Œç›´æ¥æ‰§è¡Œæœç´¢
        if not self.api_client:
            analysis_result = {
                'need_search': True,
                'reason': 'æœªé…ç½®APIå®¢æˆ·ç«¯',
                'optimized_query': query,
                'bm25_weight': 0.3,
                'vector_weight': 0.7,
                'direct_answer': None
            }
            search_results = self.search(query, top_k, search_mode)
            return analysis_result, search_results
        
        # è°ƒç”¨ç»¼åˆåˆ†æAPI
        analysis_result = self.api_client.analyze_query(query)
        
        # å¦‚æœä¸éœ€è¦æœç´¢ï¼Œè¿”å›ç©ºç»“æœ
        if not analysis_result['need_search']:
            return analysis_result, []
        
        # ä½¿ç”¨ä¼˜åŒ–åçš„æŸ¥è¯¢å’Œæƒé‡è¿›è¡Œæœç´¢
        optimized_query = analysis_result['optimized_query']
        bm25_weight = analysis_result['bm25_weight']
        vector_weight = analysis_result['vector_weight']
        
        # æ‰§è¡Œæœç´¢ï¼Œä¼ é€’æƒé‡å‚æ•°
        if metadata_filter:
            search_results = self.search_with_metadata(
                optimized_query, top_k, search_mode, metadata_filter,
                bm25_weight=bm25_weight,
                vector_weight=vector_weight
            )
        else:
            search_results = self.search(
                optimized_query, top_k, search_mode,
                bm25_weight=bm25_weight,
                vector_weight=vector_weight
            )
        
        return analysis_result, search_results
    
    def generate_answer(self, query: str, context: list) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        if self.api_client:
            return self.api_client.generate_answer(query, context)
        return self._generate_basic_answer(query, context)
    
    def _generate_basic_answer(self, query: str, context: List[Dict[str, Any]]) -> str:
        """ç”ŸæˆåŸºç¡€å›ç­”"""
        answer = f"å…³äºã€Œ{query}ã€ï¼Œåœ¨å¤æ–‡ä¸­æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n\n"
        
        for i, item in enumerate(context[:3], 1):
            meta = item['metadata']
            
            # æ™ºèƒ½é€‰æ‹©æ˜¾ç¤ºåˆ†æ•°
            score_info = self._get_score_info(item)
            
            answer += f"**{i}. ã€Š{meta['book']}Â·{meta['chapter']}ã€‹**\n"
            answer += f"åŸæ–‡ï¼šã€Œ{item['content']}ã€\n"
            answer += f"è¯é¢˜ï¼š{meta['topic']} | {score_info}\n\n"
    
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        books = set(item['metadata']['book'] for item in context)
        topics = set(item['metadata']['topic'] for item in context)
        
        answer += f"ğŸ’¡ **å†…å®¹åˆ†æ**ï¼š\n"
        answer += f"- æ¶‰åŠå…¸ç±ï¼š{', '.join(books)}\n"
        answer += f"- ç›¸å…³ä¸»é¢˜ï¼š{', '.join(topics)}\n"
        answer += f"- æ£€ç´¢æ¨¡å¼ï¼šæ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ç›¸ä¼¼åº¦ï¼‰"
        
        return answer
    
    def _get_score_info(self, item: Dict[str, Any]) -> str:
        """è·å–åˆ†æ•°æ˜¾ç¤ºä¿¡æ¯"""
        # ä¼˜å…ˆæ˜¾ç¤ºé‡æ’åºåˆ†æ•°
        if 'rerank_score' in item and item['rerank_score'] > 0:
            return f"é‡æ’åºè¯„åˆ†ï¼š{item['rerank_score']:.3f}"
        
        # å…¶æ¬¡æ˜¾ç¤ºç»¼åˆåˆ†æ•°
        elif 'combined_score' in item and item['combined_score'] > 0:
            return f"ç»¼åˆè¯„åˆ†ï¼š{item['combined_score']:.3f}"
        
        # æ˜¾ç¤ºå‘é‡åˆ†æ•°
        elif 'vector_score' in item and item['vector_score'] > 0:
            return f"ç›¸ä¼¼åº¦ï¼š{item['vector_score']:.3f}"
        
        # æ˜¾ç¤ºBM25åˆ†æ•°
        elif 'bm25_score' in item and item['bm25_score'] > 0:
            return f"åŒ¹é…åº¦ï¼š{item['bm25_score']:.3f}"
        
        # é»˜è®¤æ˜¾ç¤º
        else:
            return "ç›¸å…³åº¦ï¼šé«˜"
