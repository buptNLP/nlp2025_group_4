import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
# os.environ['TRANSFORMERS_OFFLINE'] = '1'
# os.environ['HF_HUB_OFFLINE'] = '1'
import streamlit as st
# é¡µé¢é…ç½®
st.set_page_config(
    page_title="é€šç”¨å¤æ–‡æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ“œ",
    layout="wide",
    initial_sidebar_state="expanded"
)

import re
import json
import pandas as pd
import chromadb
from chromadb.utils import embedding_functions
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, asdict
import hashlib
import requests
from pathlib import Path
import jieba
from collections import defaultdict
import numpy as np
import json
import csv
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥å¯é€‰ä¾èµ–
try:
    from rank_bm25 import BM25Okapi
    HAS_BM25 = True
except ImportError:
    HAS_BM25 = False
    st.warning("âš ï¸ æœªå®‰è£… rank-bm25ï¼Œå°†æ— æ³•ä½¿ç”¨BM25æ£€ç´¢åŠŸèƒ½ã€‚è¿è¡Œ: pip install rank-bm25")

try:
    from sentence_transformers import CrossEncoder
    import torch
    torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)] 
    HAS_RERANKER = True
except ImportError:
    HAS_RERANKER = False
    st.warning("âš ï¸ æœªå®‰è£… sentence-transformersï¼Œå°†æ— æ³•ä½¿ç”¨é‡æ’åºåŠŸèƒ½ã€‚è¿è¡Œ: pip install sentence-transformers")

@dataclass
class AncientTextSegment:
    """å¤æ–‡ç‰‡æ®µæ•°æ®ç»“æ„"""
    book: str          # ä¹¦å
    chapter: str       # ç¯‡ç« 
    speaker: str       # è¯´è¯äºº
    content: str       # å†…å®¹
    topic: str         # è¯é¢˜
    segment_id: str    # ç‰‡æ®µID
    context: str       # ä¸Šä¸‹æ–‡
    metadata: Dict[str, Any]  # æ‰©å±•å…ƒæ•°æ®

class SmartTextChunker:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å—å™¨ - ä¸¥æ ¼æŒ‰è¡Œåˆ†å—ç‰ˆ"""
    
    def __init__(self):
        # å¤æ–‡å¸¸è§çš„åˆ†å¥æ ‡ç‚¹
        self.sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š']
        # å¯¹è¯æ ‡è¯†è¯
        self.dialogue_markers = [
            'æ›°', 'äº‘', 'é—®', 'ç­”', 'å¯¹', 'è°“', 'è¨€', 'æ›°ï¼š', 'é—®ï¼š', 'ç­”ï¼š'
        ]
    
    def chunk_by_semantic_units(self, text: str, max_chunk_size: int = 200) -> List[str]:
        """
        ä¸»åˆ†å—æ–¹æ³• - ä¼˜å…ˆä½¿ç”¨ä¸¥æ ¼æŒ‰è¡Œåˆ†å—
        """
        # å¯¹äºè®ºè¯­ç­‰æŒ‰è¡Œç»„ç»‡çš„æ–‡æœ¬ï¼Œç›´æ¥æŒ‰è¡Œåˆ†å—ï¼Œä¸åˆå¹¶
        if self._is_well_organized_by_lines(text):
            return self.strict_line_chunking(text)
        else:
            # å…¶ä»–æƒ…å†µä½¿ç”¨åŸæœ‰é€»è¾‘
            return self.fallback_semantic_chunking(text, max_chunk_size)
    
    def strict_line_chunking(self, text: str) -> List[str]:
        """
        ä¸¥æ ¼æŒ‰è¡Œåˆ†å— - æ¯è¡Œä¸€ä¸ªç‹¬ç«‹chunkï¼Œä¸åˆå¹¶
        """
        lines = text.strip().split('\n')
        chunks = []
        
        for line in lines:
            line = line.strip()
            if line and len(line) >= 10:  # åªè¿‡æ»¤æ˜æ˜¾çš„ç©ºè¡Œå’Œè¿‡çŸ­è¡Œ
                chunks.append(line)
        
        return chunks
    
    def fallback_semantic_chunking(self, text: str, max_chunk_size: int) -> List[str]:
        """
        å¤‡ç”¨è¯­ä¹‰åˆ†å—æ–¹æ³•ï¼ˆå½“æ–‡æœ¬ä¸æ˜¯æŒ‰è¡Œç»„ç»‡æ—¶ä½¿ç”¨ï¼‰
        """
        chunks = []
        current_chunk = ""
        
        sentences = self._split_sentences(text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += sentence
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return self._post_process_chunks(chunks)
    
    def _is_well_organized_by_lines(self, text: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯æŒ‰è¡Œç»„ç»‡çš„æ–‡æœ¬ï¼ˆè®ºè¯­æ ¼å¼æ£€æµ‹ï¼‰
        """
        lines = text.strip().split('\n')
        if len(lines) < 2:
            return False
        
        dialogue_lines = 0
        valid_lines = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            valid_lines += 1
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¸å‹çš„è®ºè¯­å¯¹è¯æ¨¡å¼
            if any(marker in line for marker in ['æ›°ï¼š', 'é—®æ›°ï¼š', 'å¯¹æ›°ï¼š', 'è°“']) or \
               line.startswith('å­æ›°') or line.startswith('å­è°“') or \
               'é—®' in line and 'æ›°' in line:
                dialogue_lines += 1
        
        # å¦‚æœè¶…è¿‡60%çš„è¡ŒåŒ…å«å¯¹è¯æ ‡è¯†ï¼Œè®¤ä¸ºæ˜¯æŒ‰è¡Œç»„ç»‡çš„
        return valid_lines > 0 and (dialogue_lines / valid_lines) > 0.6
    
    def _split_sentences(self, text: str) -> List[str]:
        """æ™ºèƒ½åˆ†å¥"""
        pattern = f"([{''.join(self.sentence_endings)}])"
        parts = re.split(pattern, text)
        
        sentences = []
        current_sentence = ""
        
        for part in parts:
            if part in self.sentence_endings:
                current_sentence += part
                sentences.append(current_sentence)
                current_sentence = ""
            else:
                current_sentence += part
        
        if current_sentence.strip():
            sentences.append(current_sentence)
        
        return sentences
    
    def _post_process_chunks(self, chunks: List[str]) -> List[str]:
        """åå¤„ç†åˆ†å—ç»“æœ"""
        processed = []
        for chunk in chunks:
            if len(chunk.strip()) >= 5:  # åªè¿‡æ»¤æçŸ­çš„å—
                processed.append(chunk.strip())
        return processed

class AncientTextAnalyzer:
    """å¤æ–‡åˆ†æå™¨ - é€šç”¨åŒ–è®¾è®¡"""
    
    def __init__(self):
        self.speaker_patterns = self._load_speaker_patterns()
        self.topic_keywords = self._load_topic_keywords()
    
    def _load_speaker_patterns(self) -> Dict[str, List[str]]:
        """åŠ è½½è¯´è¯äººè¯†åˆ«æ¨¡å¼ - å¯æ‰©å±•"""
        return {
            # è®ºè¯­æ¨¡å¼
            'lunyu': {
                'å­”å­': ['å­æ›°', 'å­”å­æ›°', 'å­”å­è°“', 'å­è°“'],
                'å¼Ÿå­': ['æœ‰å­æ›°', 'æ›¾å­æ›°', 'å­å¤æ›°', 'å­æ¸¸æ›°', 'å­è´¡æ›°', 'å­è·¯æ›°', 'é¢œæ¸Šæ›°'],
                'é—®è€…': ['é—®æ›°', 'é—®äº', 'é—®ï¼š']
            },
            # å­Ÿå­æ¨¡å¼
            'mengzi': {
                'å­Ÿå­': ['å­Ÿå­æ›°', 'å­Ÿå­è°“'],
                'å¯¹è¯è€…': ['ç‹é—®', 'å…¬é—®', 'æˆ–é—®']
            },
            # é€šç”¨æ¨¡å¼
            'general': {
                'ä½œè€…': ['æ›°', 'äº‘', 'è¨€'],
                'å¼•ç”¨': ['ã€Š', 'ç»æ›°', 'ä¼ æ›°'],
                'å¯¹è¯': ['é—®', 'ç­”', 'å¯¹']
            }
        }
    
    def _load_topic_keywords(self) -> Dict[str, List[str]]:
        """åŠ è½½è¯é¢˜å…³é”®è¯ - å¯æ‰©å±•"""
        return {
            'å­¦ä¹ æ•™è‚²': ['å­¦', 'ä¹ ', 'æ•™', 'è¯²', 'çŸ¥', 'æ™º', 'é—®', 'æ€', 'å­¦è€Œ', 'æ•™å­¦'],
            'å“å¾·ä¿®å…»': ['ä»', 'ä¹‰', 'ç¤¼', 'æ™º', 'ä¿¡', 'å¾·', 'å–„', 'ä¿®', 'å…»', 'å“'],
            'æ”¿æ²»æ²»ç†': ['æ”¿', 'å›', 'è‡£', 'æ°‘', 'å›½', 'æ²»', 'é‚¦', 'ç‹', 'å¤©ä¸‹', 'æœ'],
            'äººé™…å…³ç³»': ['å‹', 'æœ‹', 'äº¤', 'äºº', 'äº²', 'ç¾¤', 'å’Œ', 'ç¦', 'ä¿¡ä»»'],
            'äººç”Ÿå“²å­¦': ['é“', 'å¤©', 'å‘½', 'ç”Ÿ', 'æ­»', 'ä¹', 'å¿§', 'å¿—', 'ç†æƒ³'],
            'ç¤¾ä¼šç¤¼ä»ª': ['ç¤¼', 'ä¹', 'ç¥­', 'ä¸§', 'å©š', 'å† ', 'ä»ª', 'ä¿—'],
            'ç»æµç”Ÿæ´»': ['è´¢', 'è´§', 'åˆ©', 'å•†', 'å†œ', 'å·¥', 'è´¸', 'å¯Œ'],
            'å†›äº‹æˆ˜äº‰': ['å…µ', 'æˆ˜', 'å†›', 'æ­¦', 'å¾', 'ä¼', 'å®ˆ', 'æ”»'],
            'æ–‡å­¦è‰ºæœ¯': ['è¯—', 'ä¹¦', 'ç”»', 'ä¹', 'æ–‡', 'è¯', 'èµ‹', 'é›…'],
            'è‡ªç„¶å¤©è±¡': ['å¤©', 'åœ°', 'æ—¥', 'æœˆ', 'æ˜Ÿ', 'é›¨', 'é£', 'å±±', 'æ°´']
        }
    
    def identify_speaker(self, text: str, book_type: str = 'general') -> str:
        """è¯†åˆ«è¯´è¯äºº - æ”¯æŒä¸åŒå¤æ–‡ç±»å‹"""
        text = text.strip()
        
        # æ ¹æ®ä¹¦ç±ç±»å‹é€‰æ‹©æ¨¡å¼
        patterns = self.speaker_patterns.get(book_type, self.speaker_patterns['general'])
        
        for speaker_type, speaker_patterns in patterns.items():
            for pattern in speaker_patterns:
                if text.startswith(pattern):
                    # è¿›ä¸€æ­¥ç»†åŒ–è¯†åˆ«
                    if speaker_type == 'å¼Ÿå­' and pattern in ['æœ‰å­æ›°', 'æ›¾å­æ›°']:
                        return pattern.replace('æ›°', '')
                    elif speaker_type == 'å­”å­' or pattern == 'å­æ›°':
                        return 'å­”å­'
                    else:
                        return speaker_type
        
        # é€šç”¨å¯¹è¯æ¨¡å¼è¯†åˆ«
        dialogue_match = re.search(r'([^é—®æ›°è°“]{1,10})[é—®æ›°è°“]', text)
        if dialogue_match:
            return dialogue_match.group(1)
        
        return 'ä½œè€…'
    
    def classify_topic(self, text: str) -> str:
        """åˆ†ç±»è¯é¢˜ - åŸºäºå…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰åˆ†æ"""
        topic_scores = defaultdict(int)
        
        # å…³é”®è¯åŒ¹é…
        for topic, keywords in self.topic_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    topic_scores[topic] += 1
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„è¯é¢˜
        if topic_scores:
            return max(topic_scores.items(), key=lambda x: x[1])[0]
        
        return 'å…¶ä»–'
    
    def extract_context(self, text: str, full_text: str, window_size: int = 100) -> str:
        """æå–ä¸Šä¸‹æ–‡"""
        try:
            start_idx = full_text.find(text)
            if start_idx == -1:
                return text
            
            context_start = max(0, start_idx - window_size)
            context_end = min(len(full_text), start_idx + len(text) + window_size)
            context = full_text[context_start:context_end]
            
            # æ ‡è®°å½“å‰æ–‡æœ¬
            context = context.replace(text, f"**{text}**")
            return context
        except:
            return text
        
class BGEReranker:
    """BGEé‡æ’åºå™¨"""
    
    def __init__(self, model_name: str = "BAAI/bge-reranker-base"):
        self.model = None
        self.model_name = model_name
        self.is_loaded = False
        
        if HAS_RERANKER:
            self._load_model()
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        try:
            # æ˜¾ç¤ºæ›´è¯¦ç»†çš„åŠ è½½ä¿¡æ¯
            with st.spinner(f"æ­£åœ¨åŠ è½½é‡æ’åºæ¨¡å‹ {self.model_name}..."):
                st.info(f"ğŸ¤– åŠ è½½é‡æ’åºæ¨¡å‹: {self.model_name}")
                self.model = CrossEncoder(self.model_name)
                self.is_loaded = True
                st.success(f"âœ… é‡æ’åºæ¨¡å‹ {self.model_name} åŠ è½½æˆåŠŸ")
        except Exception as e:
            st.error(f"âŒ é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            st.info("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…: pip install sentence-transformers torch")
            self.is_loaded = False
    
    def rerank(self, query: str, candidates: List[Dict], top_k: int = 5) -> List[Dict]:
        """é‡æ’åºå€™é€‰ç»“æœ"""
        if not self.is_loaded or not self.model:
            st.warning("é‡æ’åºæ¨¡å‹æœªåŠ è½½ï¼Œè¿”å›åŸå§‹ç»“æœ")
            return candidates[:top_k]
        
        try:
            # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
            pairs = [(query, candidate['content']) for candidate in candidates]
            
            # è·å–é‡æ’åºåˆ†æ•°
            scores = self.model.predict(pairs)
            
            # æ›´æ–°å€™é€‰ç»“æœçš„åˆ†æ•°
            for i, candidate in enumerate(candidates):
                candidate['rerank_score'] = float(scores[i])
            
            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            reranked_results = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)
            
            return reranked_results[:top_k]
            
        except Exception as e:
            st.error(f"é‡æ’åºè¿‡ç¨‹å‡ºé”™: {e}")
            return candidates[:top_k]

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ - BM25 + å‘é‡æ£€ç´¢"""
    
    def __init__(self, vector_collection, segments: List[AncientTextSegment]):
        self.vector_collection = vector_collection
        self.segments = segments
        self.bm25 = None
        self.segment_map = {seg.segment_id: seg for seg in segments}
        self._build_bm25_index()
        
    
    def _build_bm25_index(self):
        """æ„å»ºBM25ç´¢å¼•"""
        try:
            # åˆ†è¯
            corpus = []
            for segment in self.segments:
                tokens = list(jieba.cut(segment.content))
                corpus.append(tokens)
            
            if corpus and HAS_BM25:
                self.bm25 = BM25Okapi(corpus)
        except Exception as e:
            st.error(f"æ„å»ºBM25ç´¢å¼•å¤±è´¥: {e}")
            self.bm25 = None
    
    def hybrid_search(self, query: str, top_k: int = 10, 
                     bm25_weight: float = None, vector_weight: float = None) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢ - åŠ¨æ€æ£€æŸ¥é‡æ’åºè®¾ç½®"""
        
        # åŠ¨æ€è·å–é‡æ’åºè®¾ç½®
        use_reranker = st.session_state.get('use_reranker', False) and HAS_RERANKER
        
        if bm25_weight is None or vector_weight is None:
            bm25_w, vector_w = self._adaptive_weights(query)
        else:
            bm25_w, vector_w = bm25_weight, vector_weight
        
        # åŠ¨æ€é‡æ’åº
        if use_reranker:
            return self._hybrid_search_with_rerank(
                query, top_k, initial_k=min(50, top_k*3), 
                bm25_weight=bm25_w, vector_weight=vector_w
            )
        
        # åŸæœ‰é€»è¾‘
        if not HAS_BM25 or not self.bm25:
            return self._vector_search(query, top_k)
        
        bm25_results = self._bm25_search(query, top_k * 2)
        vector_results = self._vector_search(query, top_k * 2)
        combined_results = self._combine_results(
            bm25_results, vector_results, bm25_w, vector_w
        )
        
        return combined_results[:top_k]
    
    def _hybrid_search_with_rerank(self, query: str, top_k: int = 10, 
                                 initial_k: int = 50,
                                 bm25_weight: float = 0.3, 
                                 vector_weight: float = 0.7) -> List[Dict[str, Any]]:
        """å¸¦é‡æ’åºçš„æ··åˆæ£€ç´¢ - åŠ¨æ€åŠ è½½é‡æ’åºå™¨"""
        
        # è·å–å€™é€‰ç»“æœ
        if not HAS_BM25 or not self.bm25:
            candidates = self._vector_search(query, initial_k)
        else:
            bm25_results = self._bm25_search(query, initial_k)
            vector_results = self._vector_search(query, initial_k)
            candidates = self._combine_results(
                bm25_results, vector_results, bm25_weight, vector_weight
            )
        
        # åŠ¨æ€åˆ›å»ºé‡æ’åºå™¨
        model_name = st.session_state.get('reranker_model', 'BAAI/bge-reranker-base')
        reranker = BGEReranker(model_name)
        
        if reranker.is_loaded:
            return reranker.rerank(query, candidates[:initial_k], top_k)
        else:
            return candidates[:top_k]
    
    def _adaptive_weights(self, query: str) -> Tuple[float, float]:
        """è‡ªé€‚åº”æƒé‡è°ƒèŠ‚"""
        # å®ä½“æŸ¥è¯¢æ£€æµ‹ï¼ˆåŒ…å«å…·ä½“çš„äººåã€æ¦‚å¿µç­‰ï¼‰
        entity_keywords = ['å­”å­', 'å­Ÿå­', 'ä»', 'ä¹‰', 'ç¤¼', 'æ™º', 'ä¿¡', 'å›å­', 'å°äºº']
        
        # æ¦‚å¿µæŸ¥è¯¢æ£€æµ‹
        concept_keywords = ['å¦‚ä½•', 'ä»€ä¹ˆæ˜¯', 'ä¸ºä»€ä¹ˆ', 'æ€æ ·', 'æ–¹æ³•', 'æ€åº¦', 'æ€æƒ³']
        
        entity_score = sum(1 for keyword in entity_keywords if keyword in query)
        concept_score = sum(1 for keyword in concept_keywords if keyword in query)
        
        if entity_score > concept_score:
            # å®ä½“æŸ¥è¯¢ï¼šæé«˜BM25æƒé‡
            return 0.6, 0.4
        elif concept_score > entity_score:
            # æ¦‚å¿µæŸ¥è¯¢ï¼šæé«˜å‘é‡æƒé‡
            return 0.2, 0.8
        else:
            # å¹³è¡¡æŸ¥è¯¢ï¼šä½¿ç”¨é»˜è®¤æƒé‡
            return 0.3, 0.7
    
    def _bm25_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """BM25æ£€ç´¢"""
        if not self.bm25 or not HAS_BM25:
            return []
        
        try:
            query_tokens = list(jieba.cut(query))
            scores = self.bm25.get_scores(query_tokens)
            
            # è·å–top_kç»“æœ
            top_indices = np.argsort(scores)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if idx < len(self.segments):
                    segment = self.segments[idx]
                    results.append({
                        'segment_id': segment.segment_id,
                        'content': segment.content,
                        'metadata': asdict(segment),
                        'bm25_score': float(scores[idx]),
                        'vector_score': 0.0
                    })
            
            return results
        except Exception as e:
            st.error(f"BM25æ£€ç´¢é”™è¯¯: {e}")
            return []
    
    def _vector_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """å‘é‡æ£€ç´¢"""
        try:
            results = self.vector_collection.query(
                query_texts=[query],
                n_results=min(top_k, self.vector_collection.count())
            )
            
            vector_results = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    segment_id = results['ids'][0][i]
                    segment = self.segment_map.get(segment_id)
                    if segment:
                        similarity = 1 - results['distances'][0][i] if results['distances'] else 1.0
                        vector_results.append({
                            'segment_id': segment_id,
                            'content': doc,
                            'metadata': asdict(segment),
                            'bm25_score': 0.0,
                            'vector_score': float(similarity)
                        })
            
            return vector_results
        except Exception as e:
            st.error(f"å‘é‡æ£€ç´¢é”™è¯¯: {e}")
            return []
    
    def _combine_results(self, bm25_results: List[Dict], vector_results: List[Dict],
                        bm25_weight: float, vector_weight: float) -> List[Dict[str, Any]]:
        """åˆå¹¶æ£€ç´¢ç»“æœ"""
        # å½’ä¸€åŒ–åˆ†æ•°
        bm25_results = self._normalize_scores(bm25_results, 'bm25_score')
        vector_results = self._normalize_scores(vector_results, 'vector_score')
        
        # åˆå¹¶ç»“æœ
        combined = {}
        
        for result in bm25_results:
            sid = result['segment_id']
            combined[sid] = result.copy()
            combined[sid]['combined_score'] = result['bm25_score'] * bm25_weight
        
        for result in vector_results:
            sid = result['segment_id']
            if sid in combined:
                combined[sid]['vector_score'] = result['vector_score']
                combined[sid]['combined_score'] += result['vector_score'] * vector_weight
            else:
                combined[sid] = result.copy()
                combined[sid]['combined_score'] = result['vector_score'] * vector_weight
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
        sorted_results = sorted(combined.values(), 
                              key=lambda x: x['combined_score'], reverse=True)
        
        return sorted_results
    
    def _normalize_scores(self, results: List[Dict], score_key: str) -> List[Dict]:
        """å½’ä¸€åŒ–åˆ†æ•°"""
        if not results:
            return results
        
        scores = [r[score_key] for r in results]
        max_score = max(scores) if scores else 1.0
        min_score = min(scores) if scores else 0.0
        
        if max_score == min_score:
            return results
        
        for result in results:
            result[score_key] = (result[score_key] - min_score) / (max_score - min_score)
        
        return results

class UniversalAncientRAG:
    """é€šç”¨å¤æ–‡RAGç³»ç»Ÿ"""
    
    def __init__(self, embedding_model: str = "BAAI/bge-large-zh-v1.5"):
        self.client = chromadb.Client()
        self.collection_name = "ancient_texts_collection"
        self.embedding_model = embedding_model
        self.segments: List[AncientTextSegment] = []
        self.chunker = SmartTextChunker()
        self.analyzer = AncientTextAnalyzer()
        self.retriever: Optional[HybridRetriever] = None
        
        # é…ç½®embeddingå‡½æ•°
        self._setup_embedding_function()
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._initialize_collection()
    
    def _setup_embedding_function(self):
        """å¯ç”¨GPUåŠ é€Ÿçš„embedding"""
        try:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            st.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
            
            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name=self.embedding_model,
                device=device  # å…³é”®ï¼šæŒ‡å®šGPU
            )
        except Exception as e:
            st.warning(f"GPUåŠ é€Ÿå¤±è´¥ï¼Œä½¿ç”¨CPU: {e}")
    
    def _initialize_collection(self):
        """åˆå§‹åŒ–collection"""
        try:
            # å°è¯•è·å–ç°æœ‰collection
            self.collection = self.client.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function
            )
        except:
            # åˆ›å»ºæ–°collection
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"description": f"å¤æ–‡å‘é‡æ•°æ®åº“ - {self.embedding_model}"}
            )

    def change_embedding_model(self, new_model: str):
        """æ›´æ¢embeddingæ¨¡å‹å¹¶é‡å»ºç´¢å¼•"""
        st.info(f"ğŸ”„ åˆ‡æ¢embeddingæ¨¡å‹: {self.embedding_model} â†’ {new_model}")
        
        # ä¿å­˜å½“å‰æ•°æ®
        old_segments = self.segments.copy()
        
        # åˆ é™¤æ—§collection
        try:
            self.client.delete_collection(self.collection_name)
        except:
            pass
        
        # æ›´æ–°æ¨¡å‹
        self.embedding_model = new_model
        self._setup_embedding_function()
        self._initialize_collection()
        
        # é‡å»ºç´¢å¼•
        if old_segments:
            self.segments = old_segments
            self._build_vector_database()
            st.success(f"âœ… å·²åˆ‡æ¢åˆ° {new_model} å¹¶é‡å»ºç´¢å¼•")

    def load_from_directory(self, root_dir: str, file_extensions: List[str] = None) -> int:
        """é€’å½’åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰å¤æ–‡æ•°æ®"""
        if file_extensions is None:
            file_extensions = ['.txt', '.md', '.text']
        
        root_path = Path(root_dir)
        total_segments = 0
        
        if not root_path.exists():
            st.error(f"ç›®å½•ä¸å­˜åœ¨: {root_dir}")
            return 0
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰æŒ‡å®šæ ¼å¼çš„æ–‡ä»¶
        progress_bar = st.progress(0, text="æ­£åœ¨æ‰«ææ–‡ä»¶...")
        
        all_text_files = []
        for ext in file_extensions:
            pattern = f"**/*{ext}"
            files = list(root_path.glob(pattern))
            all_text_files.extend(files)
        
        # å»é‡å¹¶æ’åº
        all_text_files = sorted(list(set(all_text_files)))
        
        if not all_text_files:
            st.warning(f"åœ¨æŒ‡å®šç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(file_extensions)}")
            return 0
        
        st.info(f"å‘ç° {len(all_text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        
        # åˆ›å»ºå¤„ç†ç»Ÿè®¡
        processing_stats = {
            'processed_files': 0,
            'empty_files': 0,
            'error_files': 0,
            'total_segments': 0
        }
        
        # å¤„ç†æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶
        for file_idx, text_file in enumerate(all_text_files):
            progress_bar.progress(
                file_idx / len(all_text_files), 
                text=f"æ­£åœ¨å¤„ç†: {text_file.name} ({file_idx + 1}/{len(all_text_files)})"
            )
            
            try:
                # è§£ææ–‡ä»¶è·¯å¾„ä»¥è·å–ä¹¦åå’Œç¯‡ç« ä¿¡æ¯
                book_name, chapter_name = self._parse_file_path(text_file, root_path)
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                content = self._read_text_file(text_file)
                
                if content and len(content.strip()) > 10:  # å¿½ç•¥è¿‡çŸ­çš„æ–‡ä»¶
                    segments = self._process_chapter(book_name, chapter_name, content, str(text_file))
                    self.segments.extend(segments)
                    total_segments += len(segments)
                    processing_stats['total_segments'] += len(segments)
                    processing_stats['processed_files'] += 1
                    
                    # å®æ—¶æ˜¾ç¤ºå¤„ç†ç»“æœ
                    if len(segments) > 0:
                        st.sidebar.success(f"âœ… {book_name}/{chapter_name}: {len(segments)} æ®µ")
                else:
                    processing_stats['empty_files'] += 1
                    st.sidebar.warning(f"âš ï¸ ç©ºæ–‡ä»¶: {text_file.name}")
                    
            except Exception as e:
                processing_stats['error_files'] += 1
                st.sidebar.error(f"âŒ å¤„ç†å¤±è´¥: {text_file.name} - {str(e)}")
        
        # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
        st.sidebar.markdown("### ğŸ“Š å¤„ç†ç»Ÿè®¡")
        st.sidebar.metric("æˆåŠŸå¤„ç†", processing_stats['processed_files'])
        st.sidebar.metric("ç©ºæ–‡ä»¶", processing_stats['empty_files'])
        st.sidebar.metric("é”™è¯¯æ–‡ä»¶", processing_stats['error_files'])
        st.sidebar.metric("æ€»æ–‡æœ¬æ®µ", processing_stats['total_segments'])
        
        # å®Œæˆæ•°æ®åŠ è½½
        progress_bar.progress(1.0, text="æ„å»ºå‘é‡æ•°æ®åº“...")
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        if self.segments:
            self._build_vector_database()
            progress_bar.progress(1.0, text=f"åŠ è½½å®Œæˆï¼å…±å¤„ç† {total_segments} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        else:
            progress_bar.progress(1.0, text="æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®")
        
        return total_segments
    
    def load_single_directory(self, root_dir: str, file_extensions: List[str] = None) -> int:
        """åŠ è½½å•ä¸ªç›®å½•ï¼ˆéé€’å½’ï¼‰çš„å¤æ–‡æ•°æ®"""
        if file_extensions is None:
            file_extensions = ['.txt', '.md', '.text']
        
        root_path = Path(root_dir)
        total_segments = 0
        
        if not root_path.exists():
            st.error(f"ç›®å½•ä¸å­˜åœ¨: {root_dir}")
            return 0
        
        # æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶
        progress_bar = st.progress(0, text="æ­£åœ¨æ‰«ææ–‡ä»¶...")
        
        all_text_files = []
        for ext in file_extensions:
            files = list(root_path.glob(f"*{ext}"))
            all_text_files.extend(files)
        
        # ä¹Ÿæ£€æŸ¥ç›´æ¥å­ç›®å½•ä¸­çš„æ–‡ä»¶
        for subdir in root_path.iterdir():
            if subdir.is_dir():
                for ext in file_extensions:
                    files = list(subdir.glob(f"*{ext}"))
                    all_text_files.extend(files)
        
        all_text_files = sorted(list(set(all_text_files)))
        
        if not all_text_files:
            st.warning(f"åœ¨æŒ‡å®šç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(file_extensions)}")
            return 0
        
        st.info(f"å‘ç° {len(all_text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        
        # å¤„ç†æ–‡ä»¶çš„é€»è¾‘ä¸é€’å½’ç‰ˆæœ¬ç›¸åŒ
        for file_idx, text_file in enumerate(all_text_files):
            progress_bar.progress(
                file_idx / len(all_text_files), 
                text=f"æ­£åœ¨å¤„ç†: {text_file.name} ({file_idx + 1}/{len(all_text_files)})"
            )
            
            try:
                book_name, chapter_name = self._parse_file_path(text_file, root_path)
                content = self._read_text_file(text_file)
                
                if content and len(content.strip()) > 10:
                    segments = self._process_chapter(book_name, chapter_name, content, str(text_file))
                    self.segments.extend(segments)
                    total_segments += len(segments)
                    
                    if len(segments) > 0:
                        st.sidebar.success(f"âœ… {book_name}/{chapter_name}: {len(segments)} æ®µ")
                        
            except Exception as e:
                st.sidebar.error(f"âŒ å¤„ç†å¤±è´¥: {text_file.name} - {str(e)}")
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        if self.segments:
            progress_bar.progress(1.0, text="æ„å»ºå‘é‡æ•°æ®åº“...")
            self._build_vector_database()
        
        progress_bar.progress(1.0, text=f"åŠ è½½å®Œæˆï¼å…±å¤„ç† {total_segments} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        return total_segments
    
    def _parse_file_path(self, file_path: Path, root_path: Path) -> Tuple[str, str]:
        """æ™ºèƒ½è§£ææ–‡ä»¶è·¯å¾„ä»¥è·å–ä¹¦åå’Œç¯‡ç« ä¿¡æ¯"""
        try:
            # è·å–ç›¸å¯¹äºæ ¹ç›®å½•çš„è·¯å¾„
            relative_path = file_path.relative_to(root_path)
            path_parts = list(relative_path.parts[:-1])  # æ’é™¤æ–‡ä»¶å
            
            # æ ¹æ®ç›®å½•å±‚æ¬¡æ™ºèƒ½è§£æ
            if len(path_parts) == 0:
                # ç›´æ¥åœ¨æ ¹ç›®å½•ï¼štext.txt
                book_name = root_path.name
                chapter_name = file_path.stem
            elif len(path_parts) == 1:
                # ä¸€å±‚ç›®å½•ï¼šä¹¦å/text.txt æˆ– ç¯‡ç« /text.txt
                book_name = path_parts[0]
                chapter_name = file_path.stem if file_path.stem != 'text' else path_parts[0]
            elif len(path_parts) == 2:
                # æ ‡å‡†ç»“æ„ï¼šä¹¦å/ç¯‡ç« /text.txt
                book_name = path_parts[0]
                chapter_name = path_parts[1]
            else:
                # æ·±å±‚ç»“æ„ï¼šä¹¦å/å·/ç¯‡ç« /å­ç« èŠ‚/text.txt
                book_name = path_parts[0]
                # å°†ä¸­é—´å±‚çº§ç”¨å±‚æ¬¡åˆ†éš”ç¬¦è¿æ¥
                chapter_parts = path_parts[1:]
                chapter_name = " > ".join(chapter_parts)
            
            # æ¸…ç†åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            book_name = self._clean_name(book_name)
            chapter_name = self._clean_name(chapter_name)
            
            return book_name, chapter_name
            
        except Exception as e:
            # å‡ºé”™æ—¶ä½¿ç”¨é»˜è®¤å€¼
            return root_path.name, file_path.stem
    
    def _clean_name(self, name: str) -> str:
        """æ¸…ç†æ–‡ä»¶/ç›®å½•åç§°"""
        # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€
        name = re.sub(r'^\d+[-._]\s*', '', name)
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        name = re.sub(r'[_-]+', ' ', name)
        return name.strip()
    
    def _process_chapter(self, book: str, chapter: str, content: str, file_path: str = "") -> List[AncientTextSegment]:
        """å¤„ç†å•ä¸ªç¯‡ç« ï¼ˆä¸¥æ ¼æŒ‰è¡Œåˆ†å—ç‰ˆï¼‰"""
        segments = []
        
        # ä½¿ç”¨ä¸¥æ ¼æŒ‰è¡Œåˆ†å—ï¼Œä¸åˆå¹¶
        chunks = self.chunker.chunk_by_semantic_units(content)
        
        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) < 5:  # è·³è¿‡è¿‡çŸ­çš„å—
                continue
                
            # åˆ†ææ–‡æœ¬
            speaker = self.analyzer.identify_speaker(chunk, book.lower())
            topic = self.analyzer.classify_topic(chunk)
            context = self.analyzer.extract_context(chunk, content)
            
            # åˆ›å»ºç‰‡æ®µIDï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„å‘½å
            segment_id = f"{book}_{chapter}_{i:03d}"
            
            # æ‰©å±•å…ƒæ•°æ®
            metadata = {
                'length': len(chunk),
                'position': i,
                'book_type': self._detect_book_type(book),
                'file_path': file_path,
                'chunk_count': len(chunks),
                'has_dialogue': 'æ›°' in chunk or 'é—®' in chunk,
                'classical_terms': self._extract_classical_terms(chunk),
                'chunking_method': 'strict_line_based',  # æ ‡è®°ä¸ºä¸¥æ ¼æŒ‰è¡Œåˆ†å—
                'original_line': True  # æ ‡è®°ä¸ºä¿æŒåŸå§‹è¡Œç»“æ„
            }
            
            segment = AncientTextSegment(
                book=book,
                chapter=chapter,
                speaker=speaker,
                content=chunk,
                topic=topic,
                segment_id=segment_id,
                context=context,
                metadata=metadata
            )
            segments.append(segment)
        
        return segments
    
    def _extract_classical_terms(self, text: str) -> List[str]:
        """æå–å¤å…¸æœ¯è¯­"""
        classical_terms = [
            'ä»', 'ä¹‰', 'ç¤¼', 'æ™º', 'ä¿¡', 'å¾·', 'é“', 'å¤©', 'å›å­', 'å°äºº',
            'å­¦', 'ä¹ ', 'æ•™', 'è¯²', 'æ”¿', 'æ²»', 'æ°‘', 'å›½', 'å®¶', 'å­',
            'æ‚Œ', 'å¿ ', 'æ•', 'è¯š', 'æ­£', 'ä¿®', 'é½', 'æ²»', 'å¹³'
        ]
        
        found_terms = []
        for term in classical_terms:
            if term in text:
                found_terms.append(term)
        
        return found_terms
    
    def _read_text_file(self, file_path: Path) -> str:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # å¤„ç†å¯èƒ½çš„XMLæ ‡ç­¾
            content_match = re.search(r'<content>(.*?)</content>', content, re.DOTALL)
            if content_match:
                return content_match.group(1).strip()
            
            return content
        except Exception as e:
            st.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    
    def _detect_book_type(self, book_name: str) -> str:
        """æ£€æµ‹ä¹¦ç±ç±»å‹"""
        book_types = {
            'è®ºè¯­': 'lunyu',
            'å­Ÿå­': 'mengzi',
            'å¤§å­¦': 'general',
            'ä¸­åº¸': 'general'
        }
        return book_types.get(book_name, 'general')
    
    
    def _build_vector_database(self):
        """æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆä¿®å¤åˆ é™¤é”™è¯¯ï¼‰"""
        # æ¸…ç©ºæ—§æ•°æ® - ä¿®å¤ç‰ˆæœ¬
        try:
            if self.collection.count() > 0:
                # æ–¹æ³•1ï¼šè·å–æ‰€æœ‰IDç„¶ååˆ é™¤
                all_data = self.collection.get()
                if all_data['ids']:
                    self.collection.delete(ids=all_data['ids'])
            
            # æˆ–è€…ä½¿ç”¨æ–¹æ³•2ï¼šé‡æ–°åˆ›å»ºcollectionï¼ˆæ›´ç®€å•ï¼‰
            # try:
            #     self.client.delete_collection(self.collection_name)
            # except:
            #     pass  # collectionå¯èƒ½ä¸å­˜åœ¨
            # self.collection = self.client.create_collection(
            #     name=self.collection_name,
            #     metadata={"description": "é€šç”¨å¤æ–‡å‘é‡æ•°æ®åº“"}
            # )
            
        except Exception as e:
            # å¦‚æœåˆ é™¤å¤±è´¥ï¼Œå°è¯•é‡æ–°åˆ›å»ºcollection
            st.warning(f"æ¸…ç©ºæ•°æ®åº“æ—¶å‡ºç°é—®é¢˜ï¼Œå°†é‡æ–°åˆ›å»ºï¼š{e}")
            try:
                self.client.delete_collection(self.collection_name)
            except:
                pass
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "é€šç”¨å¤æ–‡å‘é‡æ•°æ®åº“"}
            )
        
        # æ‰¹é‡æ·»åŠ 
        batch_size = 200
        for i in range(0, len(self.segments), batch_size):
            batch_segments = self.segments[i:i+batch_size]
            
            documents = [seg.content for seg in batch_segments]
            
            # è¿‡æ»¤å¹¶è½¬æ¢å…ƒæ•°æ®ï¼Œç¡®ä¿åªåŒ…å«åŸºæœ¬æ•°æ®ç±»å‹
            metadatas = []
            for seg in batch_segments:
                # åˆ›å»ºåŸºç¡€å…ƒæ•°æ®
                base_metadata = {
                    'book': seg.book,
                    'chapter': seg.chapter,
                    'speaker': seg.speaker,
                    'topic': seg.topic,
                    'segment_id': seg.segment_id
                }
                
                # å¤„ç†æ‰©å±•å…ƒæ•°æ®
                if hasattr(seg, 'metadata') and seg.metadata:
                    filtered_extended = self._filter_metadata_for_chromadb(seg.metadata)
                    base_metadata.update(filtered_extended)
                
                metadatas.append(base_metadata)
            
            ids = [seg.segment_id for seg in batch_segments]
            
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )

        if self.segments:
            # ç¡®ä¿æ­£ç¡®è·å–ç”¨æˆ·é…ç½®
            use_reranker = st.session_state.get('use_reranker', HAS_RERANKER)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            st.info(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ£€ç´¢å™¨ï¼Œé‡æ’åº: {'å¯ç”¨' if use_reranker else 'ç¦ç”¨'}")
            
            self.retriever = HybridRetriever(
                self.collection, 
                self.segments, 
                # use_reranker=use_reranker  # ç¡®ä¿æ­£ç¡®ä¼ é€’å‚æ•°
            )
            
            # æ˜¾ç¤ºé‡æ’åºçŠ¶æ€
            st.info("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
            
    
    def _filter_metadata_for_chromadb(self, metadata: Dict) -> Dict:
        """è¿‡æ»¤å…ƒæ•°æ®ï¼Œç¡®ä¿åªåŒ…å«ChromaDBæ”¯æŒçš„æ•°æ®ç±»å‹"""
        filtered = {}
        
        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool)):
                # åŸºæœ¬ç±»å‹ç›´æ¥ä¿ç•™
                filtered[key] = value
            elif isinstance(value, list):
                # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²å’Œè®¡æ•°
                if value:  # éç©ºåˆ—è¡¨
                    filtered[f"{key}_str"] = ', '.join(map(str, value))
                    filtered[f"{key}_count"] = len(value)
                else:
                    filtered[f"{key}_count"] = 0
            elif isinstance(value, dict):
                # å°†å­—å…¸è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                filtered[f"{key}_str"] = str(value)
            else:
                # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                filtered[f"{key}_str"] = str(value)
        
        return filtered
        
    def search(self, query: str, top_k: int = 5, 
              search_mode: str = 'hybrid') -> List[Dict[str, Any]]:
        """æœç´¢åŠŸèƒ½"""
        if not self.retriever:
            st.error("æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåŠ è½½æ•°æ®")
            return []
        
        try:
            if search_mode == 'hybrid':
                if HAS_BM25:
                    return self.retriever.hybrid_search(query, top_k)
                else:
                    st.info("BM25ä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å‘é‡æ£€ç´¢æ¨¡å¼")
                    return self.retriever._vector_search(query, top_k)
            elif search_mode == 'vector':
                return self.retriever._vector_search(query, top_k)
            elif search_mode == 'bm25':
                if HAS_BM25:
                    return self.retriever._bm25_search(query, top_k)
                else:
                    st.warning("BM25æ£€ç´¢ä¸å¯ç”¨ï¼Œè¯·å®‰è£… rank-bm25 åŒ…")
                    return []
            else:
                return []
        except Exception as e:
            st.error(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return []
        
    def optimize_query(self, raw_query: str, api_key: str) -> Tuple[str, str]:
        """ä¼˜åŒ–ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢"""
        if not api_key or not api_key.strip():
            return raw_query, "æœªä½¿ç”¨APIä¼˜åŒ–"
        
        prompt = f"""ä½ æ˜¯å¤æ–‡æ£€ç´¢ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„é—®é¢˜è½¬æ¢ä¸ºæ›´é€‚åˆå¤æ–‡æ£€ç´¢çš„æŸ¥è¯¢ã€‚

    ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š{raw_query}

    è¯·æŒ‰ä»¥ä¸‹è¦æ±‚ä¼˜åŒ–ï¼š
    1. æå–æ ¸å¿ƒå…³é”®è¯ï¼ˆå¤æ–‡æœ¯è¯­ã€äººç‰©ã€æ¦‚å¿µï¼‰
    2. è¡¥å……ç›¸å…³çš„å¤å…¸è¡¨è¿°
    3. ä¿æŒé—®é¢˜çš„æ ¸å¿ƒæ„å›¾
    4. ç”Ÿæˆç®€æ´ä½†ç²¾å‡†çš„æ£€ç´¢æŸ¥è¯¢

    åªè¿”å›ä¼˜åŒ–åçš„æŸ¥è¯¢ï¼Œä¸è¦è§£é‡Šã€‚

    ä¼˜åŒ–æŸ¥è¯¢ï¼š"""

        try:
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {api_key.strip()}'
            }
            
            data = {
                'model': 'deepseek-chat',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'stream': False,
                'max_tokens': 200,
                'temperature': 0.3
            }
            
            response = requests.post(
                'https://api.deepseek.com/chat/completions',
                headers=headers,
                json=data,
                timeout=15
            )
            
            if response.status_code == 200:
                result = response.json()
                optimized_query = result['choices'][0]['message']['content'].strip()
                return optimized_query, "APIä¼˜åŒ–æˆåŠŸ"
            else:
                return raw_query, f"APIè°ƒç”¨å¤±è´¥: {response.status_code}"
                
        except Exception as e:
            return raw_query, f"ä¼˜åŒ–å¤±è´¥: {str(e)}"
    
    def generate_answer(self, query: str, context: List[Dict[str, Any]], 
                       api_key: str = None) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        if not context:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å¤æ–‡å†…å®¹ã€‚"
        
        # æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡
        context_parts = []
        for item in context[:3]:
            meta = item['metadata']
            context_parts.append(
                f"ã€{meta['book']} Â· {meta['chapter']}ã€‘{meta['speaker']}: {item['content']}"
            )
        
        context_text = "\n\n".join(context_parts)
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­å›½å¤å…¸æ–‡å­¦çš„å­¦è€…ï¼Œè¯·åŸºäºä»¥ä¸‹å¤æ–‡åŸæ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç›¸å…³å¤æ–‡åŸæ–‡ï¼š
{context_text}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š
1. é¦–å…ˆå¼•ç”¨æœ€ç›¸å…³çš„åŸæ–‡
2. è¯¦ç»†å®Œæ•´åœ°å›ç­”ç”¨æˆ·é—®é¢˜
3. è§£é‡Šå¤æ–‡çš„å­—é¢å«ä¹‰
4. é˜è¿°å…¶æ·±å±‚æ€æƒ³å†…æ¶µ
5. ç»“åˆç°ä»£è§‚ç‚¹è¿›è¡Œåˆ†æ
6. æä¾›å®é™…çš„æŒ‡å¯¼æ„ä¹‰

å›ç­”è¦æ±‚ï¼šç¬¬äºŒéƒ¨åˆ†è¯¦ç»†å›ç­”ï¼Œå…¶ä½™å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡ºï¼Œæ—¢æœ‰å­¦æœ¯æ·±åº¦åˆé€šä¿—æ˜“æ‡‚ã€‚

å›ç­”ï¼š"""

        # è°ƒç”¨APIæˆ–ç”ŸæˆåŸºç¡€å›ç­”
        if api_key and api_key.strip():
            return self._call_api(prompt, api_key.strip())
        else:
            return self._generate_basic_answer(query, context)
    
    def _call_api(self, prompt: str, api_key: str) -> str:
        """è°ƒç”¨AI API"""
        try:
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {api_key}'
            }
            
            data = {
                'model': 'deepseek-chat',
                'messages': [
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¤å…¸æ–‡å­¦å­¦è€…ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                'stream': False,
                'max_tokens': 1500,
                'temperature': 0.7
            }
            
            response = requests.post(
                'https://api.deepseek.com/chat/completions',
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                return f"APIè°ƒç”¨å¤±è´¥: {response.status_code}"
                
        except Exception as e:
            return f"APIè°ƒç”¨å‡ºé”™: {str(e)}"
    
    def _generate_basic_answer(self, query: str, context: List[Dict[str, Any]]) -> str:
        """ç”ŸæˆåŸºç¡€å›ç­”"""
        answer = f"å…³äºã€Œ{query}ã€ï¼Œåœ¨å¤æ–‡ä¸­æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n\n"
        
        for i, item in enumerate(context[:3], 1):
            meta = item['metadata']
            
            # æ™ºèƒ½é€‰æ‹©æ˜¾ç¤ºåˆ†æ•°
            score_info = self._get_score_info(item)
            
            answer += f"**{i}. ã€Š{meta['book']}Â·{meta['chapter']}ã€‹**\n"
            answer += f"åŸæ–‡ï¼šã€Œ{item['content']}ã€\n"
            answer += f"è¯é¢˜ï¼š{meta['topic']} | {score_info}\n\n"
    
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        books = set(item['metadata']['book'] for item in context)
        topics = set(item['metadata']['topic'] for item in context)
        
        answer += f"ğŸ’¡ **å†…å®¹åˆ†æ**ï¼š\n"
        answer += f"- æ¶‰åŠå…¸ç±ï¼š{', '.join(books)}\n"
        answer += f"- ç›¸å…³ä¸»é¢˜ï¼š{', '.join(topics)}\n"
        answer += f"- æ£€ç´¢æ¨¡å¼ï¼šæ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ç›¸ä¼¼åº¦ï¼‰"
        
        return answer
    
    def _get_score_info(self, item: Dict[str, Any]) -> str:
        """è·å–åˆ†æ•°æ˜¾ç¤ºä¿¡æ¯"""
        # ä¼˜å…ˆæ˜¾ç¤ºé‡æ’åºåˆ†æ•°
        if 'rerank_score' in item and item['rerank_score'] > 0:
            return f"é‡æ’åºè¯„åˆ†ï¼š{item['rerank_score']:.3f}"
        
        # å…¶æ¬¡æ˜¾ç¤ºç»¼åˆåˆ†æ•°
        elif 'combined_score' in item and item['combined_score'] > 0:
            return f"ç»¼åˆè¯„åˆ†ï¼š{item['combined_score']:.3f}"
        
        # æ˜¾ç¤ºå‘é‡åˆ†æ•°
        elif 'vector_score' in item and item['vector_score'] > 0:
            return f"ç›¸ä¼¼åº¦ï¼š{item['vector_score']:.3f}"
        
        # æ˜¾ç¤ºBM25åˆ†æ•°
        elif 'bm25_score' in item and item['bm25_score'] > 0:
            return f"åŒ¹é…åº¦ï¼š{item['bm25_score']:.3f}"
        
        # é»˜è®¤æ˜¾ç¤º
        else:
            return "ç›¸å…³åº¦ï¼šé«˜"
    
    def save_processing_results(self, output_dir: str = "./processing_results") -> bool:
        """ä¿å­˜å¤„ç†ç»“æœåˆ°æŒ‡å®šç›®å½•"""
        if not self.segments:
            st.warning("æ²¡æœ‰å¤„ç†ç»“æœå¯ä¿å­˜")
            return False
        
        try:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. ä¿å­˜è¯¦ç»†çš„JSONæ ¼å¼ç»“æœ
            self._save_detailed_json(output_path, timestamp)
            
            # 2. ä¿å­˜ä¾¿äºæŸ¥çœ‹çš„HTMLæ ¼å¼ç»“æœ
            self._save_html_report(output_path, timestamp)
            
            # 3. ä¿å­˜CSVæ ¼å¼ç»“æœï¼ˆä¾¿äºExcelæ‰“å¼€ï¼‰
            self._save_csv_report(output_path, timestamp)
            
            # 4. ä¿å­˜æŒ‰ä¹¦ç±åˆ†ç±»çš„æ–‡æœ¬æ–‡ä»¶
            self._save_by_books(output_path, timestamp)
            
            # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            self._save_statistics_report(output_path, timestamp)
            
            st.success(f"âœ… å¤„ç†ç»“æœå·²ä¿å­˜åˆ°ï¼š{output_path.absolute()}")
            return True
            
        except Exception as e:
            st.error(f"ä¿å­˜å¤„ç†ç»“æœå¤±è´¥ï¼š{str(e)}")
            return False
    
    def _save_detailed_json(self, output_path: Path, timestamp: str):
        """ä¿å­˜è¯¦ç»†çš„JSONæ ¼å¼ç»“æœ"""
        json_file = output_path / f"segments_detailed_{timestamp}.json"
        
        segments_data = []
        for seg in self.segments:
            segment_dict = asdict(seg)
            segments_data.append(segment_dict)
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(segments_data, f, ensure_ascii=False, indent=2)
        
        st.info(f"ğŸ“„ è¯¦ç»†JSONç»“æœï¼š{json_file.name}")
    
    def _save_html_report(self, output_path: Path, timestamp: str):
        """ä¿å­˜HTMLæ ¼å¼çš„å¯è§†åŒ–æŠ¥å‘Š"""
        html_file = output_path / f"processing_report_{timestamp}.html"
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å¤æ–‡å¤„ç†ç»“æœæŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 20px; margin-bottom: 30px; }}
        .stats {{ display: flex; justify-content: space-around; margin-bottom: 30px; }}
        .stat-card {{ background: #ecf0f1; padding: 15px; border-radius: 8px; text-align: center; min-width: 120px; }}
        .stat-number {{ font-size: 24px; font-weight: bold; color: #e74c3c; }}
        .segment {{ border: 1px solid #bdc3c7; margin-bottom: 15px; border-radius: 8px; overflow: hidden; }}
        .segment-header {{ background: #34495e; color: white; padding: 10px; font-weight: bold; }}
        .segment-content {{ padding: 15px; }}
        .metadata {{ background: #f8f9fa; padding: 10px; margin-top: 10px; border-radius: 4px; font-size: 0.9em; }}
        .content-text {{ line-height: 1.8; margin: 10px 0; font-size: 16px; }}
        .book-section {{ margin-bottom: 40px; }}
        .book-title {{ color: #8e44ad; font-size: 20px; font-weight: bold; margin-bottom: 15px; border-left: 4px solid #8e44ad; padding-left: 10px; }}
        .context {{ background: #fff3cd; padding: 10px; margin-top: 10px; border-radius: 4px; border-left: 4px solid #ffc107; }}
        .topic-tag {{ background: #17a2b8; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.8em; margin-right: 5px; }}
        .speaker-tag {{ background: #28a745; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.8em; margin-right: 5px; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“œ å¤æ–‡å¤„ç†ç»“æœæŠ¥å‘Š</h1>
            <p>ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number">{len(self.segments)}</div>
                <div>æ–‡æœ¬ç‰‡æ®µæ€»æ•°</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{len(set(seg.book for seg in self.segments))}</div>
                <div>ä¹¦ç±æ•°é‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{len(set(seg.chapter for seg in self.segments))}</div>
                <div>ç« èŠ‚æ•°é‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{sum(len(seg.content) for seg in self.segments)}</div>
                <div>æ€»å­—ç¬¦æ•°</div>
            </div>
        </div>
        
        {self._generate_html_segments_by_book()}
    </div>
</body>
</html>"""
        
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        st.info(f"ğŸŒ HTMLå¯è§†åŒ–æŠ¥å‘Šï¼š{html_file.name}")
    
    def _generate_html_segments_by_book(self) -> str:
        """ç”ŸæˆæŒ‰ä¹¦ç±åˆ†ç»„çš„HTMLå†…å®¹"""
        html_parts = []
        
        # æŒ‰ä¹¦ç±åˆ†ç»„
        books = {}
        for seg in self.segments:
            if seg.book not in books:
                books[seg.book] = []
            books[seg.book].append(seg)
        
        for book_name, segments in books.items():
            html_parts.append(f'<div class="book-section">')
            html_parts.append(f'<div class="book-title">ğŸ“– {book_name} ({len(segments)} ä¸ªç‰‡æ®µ)</div>')
            
            for i, seg in enumerate(segments, 1):
                html_parts.append(f'''
                <div class="segment">
                    <div class="segment-header">
                        ç‰‡æ®µ {i} - {seg.chapter} 
                        <span class="speaker-tag">{seg.speaker}</span>
                        <span class="topic-tag">{seg.topic}</span>
                    </div>
                    <div class="segment-content">
                        <div class="content-text">{seg.content}</div>
                        <div class="metadata">
                            <strong>ç‰‡æ®µIDï¼š</strong>{seg.segment_id} | 
                            <strong>å­—ç¬¦æ•°ï¼š</strong>{len(seg.content)} | 
                            <strong>ä½ç½®ï¼š</strong>{seg.metadata.get('position', 'æœªçŸ¥')}
                        </div>
                        {f'<div class="context"><strong>ä¸Šä¸‹æ–‡ï¼š</strong><br>{seg.context}</div>' if seg.context != seg.content else ''}
                    </div>
                </div>
                ''')
            
            html_parts.append('</div>')
        
        return ''.join(html_parts)
    
    def _save_csv_report(self, output_path: Path, timestamp: str):
        """ä¿å­˜CSVæ ¼å¼æŠ¥å‘Šï¼ˆä¾¿äºExcelæŸ¥çœ‹ï¼‰"""
        csv_file = output_path / f"segments_table_{timestamp}.csv"
        
        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            headers = [
                'åºå·', 'ä¹¦å', 'ç« èŠ‚', 'è¯´è¯äºº', 'è¯é¢˜', 'å†…å®¹', 
                'å­—ç¬¦æ•°', 'ç‰‡æ®µID', 'ä½ç½®', 'æ˜¯å¦å¯¹è¯', 'å¤å…¸æœ¯è¯­'
            ]
            writer.writerow(headers)
            
            # å†™å…¥æ•°æ®
            for i, seg in enumerate(self.segments, 1):
                classical_terms = seg.metadata.get('classical_terms', [])
                if isinstance(classical_terms, list):
                    terms_str = ', '.join(classical_terms)
                else:
                    terms_str = str(classical_terms)
                
                row = [
                    i,
                    seg.book,
                    seg.chapter,
                    seg.speaker,
                    seg.topic,
                    seg.content,
                    len(seg.content),
                    seg.segment_id,
                    seg.metadata.get('position', ''),
                    seg.metadata.get('has_dialogue', False),
                    terms_str
                ]
                writer.writerow(row)
        
        st.info(f"ğŸ“Š CSVè¡¨æ ¼æ–‡ä»¶ï¼š{csv_file.name}")
    
    def _save_by_books(self, output_path: Path, timestamp: str):
        """æŒ‰ä¹¦ç±ä¿å­˜åˆ†ç±»çš„æ–‡æœ¬æ–‡ä»¶"""
        books_dir = output_path / f"by_books_{timestamp}"
        books_dir.mkdir(exist_ok=True)
        
        # æŒ‰ä¹¦ç±åˆ†ç»„
        books = {}
        for seg in self.segments:
            if seg.book not in books:
                books[seg.book] = {}
            if seg.chapter not in books[seg.book]:
                books[seg.book][seg.chapter] = []
            books[seg.book][seg.chapter].append(seg)
        
        for book_name, chapters in books.items():
            book_dir = books_dir / book_name
            book_dir.mkdir(exist_ok=True)
            
            # ä¸ºæ¯ä¸ªç« èŠ‚åˆ›å»ºæ–‡ä»¶
            for chapter_name, segments in chapters.items():
                chapter_file = book_dir / f"{chapter_name}_segments.txt"
                
                with open(chapter_file, 'w', encoding='utf-8') as f:
                    f.write(f"ã€Š{book_name}ã€‹- {chapter_name}\n")
                    f.write("=" * 50 + "\n\n")
                    
                    for i, seg in enumerate(segments, 1):
                        f.write(f"ã€ç‰‡æ®µ {i}ã€‘\n")
                        f.write(f"è¯´è¯äººï¼š{seg.speaker}\n")
                        f.write(f"è¯é¢˜ï¼š{seg.topic}\n")
                        f.write(f"å†…å®¹ï¼š{seg.content}\n")
                        f.write(f"ç‰‡æ®µIDï¼š{seg.segment_id}\n")
                        f.write(f"å­—ç¬¦æ•°ï¼š{len(seg.content)}\n")
                        
                        # æ·»åŠ å¤å…¸æœ¯è¯­ä¿¡æ¯
                        terms = seg.metadata.get('classical_terms', [])
                        if terms:
                            if isinstance(terms, list):
                                f.write(f"å¤å…¸æœ¯è¯­ï¼š{', '.join(terms)}\n")
                            else:
                                f.write(f"å¤å…¸æœ¯è¯­ï¼š{terms}\n")
                        
                        f.write("-" * 30 + "\n\n")
        
        st.info(f"ğŸ“š æŒ‰ä¹¦ç±åˆ†ç±»çš„æ–‡ä»¶ï¼š{books_dir.name}/")
    
    def _save_statistics_report(self, output_path: Path, timestamp: str):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        stats_file = output_path / f"statistics_{timestamp}.txt"
        
        # æ”¶é›†ç»Ÿè®¡æ•°æ®
        total_segments = len(self.segments)
        books_stats = {}
        topics_stats = {}
        speakers_stats = {}
        length_stats = []
        
        for seg in self.segments:
            # ä¹¦ç±ç»Ÿè®¡
            books_stats[seg.book] = books_stats.get(seg.book, 0) + 1
            
            # è¯é¢˜ç»Ÿè®¡
            topics_stats[seg.topic] = topics_stats.get(seg.topic, 0) + 1
            
            # è¯´è¯äººç»Ÿè®¡
            speakers_stats[seg.speaker] = speakers_stats.get(seg.speaker, 0) + 1
            
            # é•¿åº¦ç»Ÿè®¡
            length_stats.append(len(seg.content))
        
        # è®¡ç®—é•¿åº¦ç»Ÿè®¡
        avg_length = sum(length_stats) / len(length_stats) if length_stats else 0
        min_length = min(length_stats) if length_stats else 0
        max_length = max(length_stats) if length_stats else 0
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write("å¤æ–‡å¤„ç†ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
            
            f.write("ğŸ“Š åŸºæœ¬ç»Ÿè®¡\n")
            f.write("-" * 20 + "\n")
            f.write(f"æ–‡æœ¬ç‰‡æ®µæ€»æ•°ï¼š{total_segments}\n")
            f.write(f"å¹³å‡ç‰‡æ®µé•¿åº¦ï¼š{avg_length:.1f} å­—ç¬¦\n")
            f.write(f"æœ€çŸ­ç‰‡æ®µé•¿åº¦ï¼š{min_length} å­—ç¬¦\n")
            f.write(f"æœ€é•¿ç‰‡æ®µé•¿åº¦ï¼š{max_length} å­—ç¬¦\n")
            f.write(f"æ€»å­—ç¬¦æ•°ï¼š{sum(length_stats)}\n\n")
            
            f.write("ğŸ“š ä¹¦ç±åˆ†å¸ƒ\n")
            f.write("-" * 20 + "\n")
            for book, count in sorted(books_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_segments) * 100
                f.write(f"{book}ï¼š{count} æ®µ ({percentage:.1f}%)\n")
            f.write("\n")
            
            f.write("ğŸ·ï¸ è¯é¢˜åˆ†å¸ƒ\n")
            f.write("-" * 20 + "\n")
            for topic, count in sorted(topics_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_segments) * 100
                f.write(f"{topic}ï¼š{count} æ®µ ({percentage:.1f}%)\n")
            f.write("\n")
            
            f.write("ğŸ‘¤ è¯´è¯äººåˆ†å¸ƒ\n")
            f.write("-" * 20 + "\n")
            for speaker, count in sorted(speakers_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_segments) * 100
                f.write(f"{speaker}ï¼š{count} æ®µ ({percentage:.1f}%)\n")
        
        st.info(f"ğŸ“ˆ ç»Ÿè®¡æŠ¥å‘Šï¼š{stats_file.name}")


def main():
    """ä¸»åº”ç”¨"""
    st.title("ğŸ“œ é€šç”¨å¤æ–‡æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    st.markdown("*åŸºäºæ··åˆæ£€ç´¢æŠ€æœ¯çš„æ™ºèƒ½å¤æ–‡RAGç³»ç»Ÿ*")
    st.markdown("---")
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    if 'rag_system' not in st.session_state:
        # åˆå§‹åŒ–æ—¶æŒ‡å®šä¸­æ–‡ä¼˜åŒ–çš„embeddingæ¨¡å‹
        st.session_state.rag_system = UniversalAncientRAG(
            embedding_model="BAAI/bge-large-zh-v1.5"
        )
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
        
        # æ•°æ®åŠ è½½
        st.subheader("ğŸ“š æ•°æ®ç®¡ç†")
        data_dir = st.text_input("å¤æ–‡æ•°æ®ç›®å½•è·¯å¾„", value="./è®ºè¯­")
        
        # æ–‡ä»¶æ ¼å¼é€‰æ‹©
        file_formats = st.multiselect(
            "æ”¯æŒçš„æ–‡ä»¶æ ¼å¼",
            ['.txt', '.md', '.text', '.doc'],
            default=['.txt', '.md'],
            help="é€‰æ‹©è¦åŠ è½½çš„æ–‡ä»¶æ ¼å¼"
        )
        
        # é«˜çº§é€‰é¡¹
        with st.expander("ğŸ”§ é«˜çº§é€‰é¡¹"):
            recursive_load = st.checkbox("é€’å½’åŠ è½½å­ç›®å½•", value=True, help="æ˜¯å¦å¤„ç†æ‰€æœ‰å­ç›®å½•ä¸­çš„æ–‡ä»¶")
            min_content_length = st.slider("æœ€å°å†…å®¹é•¿åº¦", 5, 100, 10, help="å¿½ç•¥è¿‡çŸ­çš„æ–‡ä»¶")
            show_processing_details = st.checkbox("æ˜¾ç¤ºå¤„ç†è¯¦æƒ…", value=True)
        
        if st.button("ğŸ”„ åŠ è½½å¤æ–‡æ•°æ®", type="primary"):
            with st.spinner("æ­£åœ¨åŠ è½½å¤æ–‡æ•°æ®..."):
                # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
                st.session_state.rag_system.segments = []
                
                if recursive_load:
                    count = st.session_state.rag_system.load_from_directory(
                        data_dir, 
                        file_extensions=file_formats
                    )
                else:
                    count = st.session_state.rag_system.load_single_directory(
                        data_dir, 
                        file_extensions=file_formats
                    )
                
                if count > 0:
                    st.success(f"âœ… æˆåŠŸåŠ è½½ {count} ä¸ªæ–‡æœ¬ç‰‡æ®µï¼")
                    st.balloons()  # æ·»åŠ åº†ç¥åŠ¨ç”»
                else:
                    st.error("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›®å½•è·¯å¾„å’Œæ–‡ä»¶æ ¼å¼")
        
        # APIé…ç½®
        st.subheader("ğŸ¤– AIé…ç½®")
        api_key = st.text_input("DeepSeek API Key", type="password")

        # APIé…ç½®éƒ¨åˆ†åæ·»åŠ 
        query_optimization = st.checkbox(
            "ğŸ” æ™ºèƒ½æŸ¥è¯¢ä¼˜åŒ–", 
            value=bool(api_key and api_key.strip()),  # æœ‰API KEYæ—¶é»˜è®¤å¼€å¯
            disabled=not bool(api_key and api_key.strip()),  # æ— API KEYæ—¶ç¦ç”¨
            help="ä½¿ç”¨AIä¼˜åŒ–ç”¨æˆ·è¾“å…¥çš„é—®é¢˜ï¼Œæé«˜æ£€ç´¢å‡†ç¡®æ€§"
        )
        st.session_state.query_optimization = query_optimization
        
        # æ£€ç´¢é…ç½®
        st.subheader("ğŸ” æ£€ç´¢è®¾ç½®")

        # Embeddingæ¨¡å‹é€‰æ‹©
        embedding_models = [
            "BAAI/bge-large-zh-v1.5",
            # "BAAI/bge-base-zh-v1.5", 
            # "text2vec-chinese",
            # "multilingual-e5-large",
            # "all-MiniLM-L6-v2"  # è‹±æ–‡æ¨¡å‹ä½œä¸ºå¯¹æ¯”
        ]
        
        selected_model = st.selectbox(
            "Embeddingæ¨¡å‹",
            embedding_models,
            help="é€‰æ‹©å‘é‡åŒ–æ¨¡å‹ï¼Œä¸­æ–‡æ¨¡å‹å¯¹å¤æ–‡æ•ˆæœæ›´å¥½"
        )
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢æ¨¡å‹
        current_model = getattr(st.session_state.rag_system, 'embedding_model', 'default')
        if selected_model != current_model:
            if st.button("ğŸ”„ åˆ‡æ¢Embeddingæ¨¡å‹"):
                with st.spinner("æ­£åœ¨åˆ‡æ¢æ¨¡å‹å¹¶é‡å»ºç´¢å¼•..."):
                    st.session_state.rag_system.change_embedding_model(selected_model)
        
        # æ˜¾ç¤ºå½“å‰æ¨¡å‹
        if hasattr(st.session_state.rag_system, 'embedding_model'):
            st.info(f"å½“å‰æ¨¡å‹: {st.session_state.rag_system.embedding_model}")
        search_mode = st.selectbox(
            "æ£€ç´¢æ¨¡å¼",
            ["hybrid", "vector", "bm25"],
            format_func=lambda x: {"hybrid": "æ··åˆæ£€ç´¢", "vector": "å‘é‡æ£€ç´¢", "bm25": "å…³é”®è¯æ£€ç´¢"}[x]
        )
        top_k = st.slider("è¿”å›ç»“æœæ•°é‡", 1, 20, 5)
        
        # æ·»åŠ é‡æ’åºé€‰é¡¹ï¼ˆå…³é”®ä¿®æ”¹ï¼‰
        if HAS_RERANKER:
            use_reranker = st.checkbox(
                "ğŸ”„ å¯ç”¨æ·±åº¦é‡æ’åº", 
                value=True, 
                help="ä½¿ç”¨BGEæ¨¡å‹è¿›è¡Œæ·±åº¦è¯­ä¹‰é‡æ’åºï¼Œæé«˜ç»“æœå‡†ç¡®æ€§"
            )
            st.session_state.use_reranker = use_reranker
            
            if use_reranker:
                reranker_model = st.selectbox(
                    "é‡æ’åºæ¨¡å‹",
                    ["BAAI/bge-reranker-base"],
                    help="é€‰æ‹©é‡æ’åºæ¨¡å‹"
                )
                st.session_state.reranker_model = reranker_model
        else:
            st.warning("âš ï¸ éœ€è¦å®‰è£… sentence-transformers æ‰èƒ½ä½¿ç”¨é‡æ’åºåŠŸèƒ½")
            st.code("pip install sentence-transformers torch")
            st.session_state.use_reranker = False
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if hasattr(st.session_state.rag_system, 'segments') and st.session_state.rag_system.segments:
            st.subheader("ğŸ“Š æ•°æ®ç»Ÿè®¡")
            segments = st.session_state.rag_system.segments
            
            # åŸºæœ¬ç»Ÿè®¡
            st.metric("æ–‡æœ¬ç‰‡æ®µæ€»æ•°", len(segments))
            
            # ä¹¦ç±åˆ†å¸ƒ
            books = defaultdict(int)
            topics = defaultdict(int)
            speakers = defaultdict(int)
            
            for seg in segments:
                books[seg.book] += 1
                topics[seg.topic] += 1
                speakers[seg.speaker] += 1
            
            st.write("ğŸ“– **ä¹¦ç±åˆ†å¸ƒ**")
            for book, count in sorted(books.items(), key=lambda x: x[1], reverse=True):
                st.write(f"- {book}: {count} æ®µ")
            
            st.write("ğŸ·ï¸ **è¯é¢˜åˆ†å¸ƒ**")
            for topic, count in sorted(topics.items(), key=lambda x: x[1], reverse=True)[:5]:
                st.write(f"- {topic}: {count} æ®µ")
        
        # åœ¨æ•°æ®ç»Ÿè®¡éƒ¨åˆ†åæ·»åŠ ä¿å­˜åŠŸèƒ½
        if hasattr(st.session_state.rag_system, 'segments') and st.session_state.rag_system.segments:
            st.subheader("ğŸ’¾ ç»“æœå¯¼å‡º")
            
            # è¾“å‡ºç›®å½•è®¾ç½®
            output_dir = st.text_input(
                "ä¿å­˜ç›®å½•", 
                value="./processing_results",
                help="å¤„ç†ç»“æœå°†ä¿å­˜åˆ°æ­¤ç›®å½•"
            )
            
            # ä¿å­˜é€‰é¡¹
            save_options = st.multiselect(
                "é€‰æ‹©ä¿å­˜æ ¼å¼",
                ["HTMLæŠ¥å‘Š", "CSVè¡¨æ ¼", "JSONæ•°æ®", "æŒ‰ä¹¦ç±åˆ†ç±»", "ç»Ÿè®¡æŠ¥å‘Š"],
                default=["HTMLæŠ¥å‘Š", "CSVè¡¨æ ¼", "ç»Ÿè®¡æŠ¥å‘Š"],
                help="é€‰æ‹©è¦ç”Ÿæˆçš„æ–‡ä»¶æ ¼å¼"
            )
            
            if st.button("ğŸ“¥ å¯¼å‡ºå¤„ç†ç»“æœ", type="secondary"):
                with st.spinner("æ­£åœ¨ä¿å­˜å¤„ç†ç»“æœ..."):
                    success = st.session_state.rag_system.save_processing_results(output_dir)
                    if success:
                        st.balloons()
                        
                        # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶ä¿¡æ¯
                        st.success("ğŸ‰ ç»“æœå¯¼å‡ºæˆåŠŸï¼")
                        st.info(f"""
                        **å·²ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š**
                        - ğŸ“„ è¯¦ç»†JSONæ•°æ®
                        - ğŸŒ HTMLå¯è§†åŒ–æŠ¥å‘Š  
                        - ğŸ“Š CSVè¡¨æ ¼æ–‡ä»¶
                        - ğŸ“š æŒ‰ä¹¦ç±åˆ†ç±»çš„æ–‡æœ¬
                        - ğŸ“ˆ ç»Ÿè®¡åˆ†ææŠ¥å‘Š
                        
                        **ä¿å­˜ä½ç½®ï¼š** `{output_dir}`
                        """)
    
    # ä¸»ç•Œé¢
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”")
        
        # é¢„è®¾é—®é¢˜
        st.subheader("ğŸ“‹ ç¤ºä¾‹é—®é¢˜")
        example_questions = [
            "ä»€ä¹ˆæ˜¯ä»ï¼Ÿ",
            "å­”å­çš„æ•™è‚²æ€æƒ³",
            "å›å­ä¸å°äººçš„åŒºåˆ«",
            "å¦‚ä½•ä¿®èº«å…»æ€§ï¼Ÿ",
            "å¤äººçš„æ”¿æ²»ç†æƒ³",
            "å­¦ä¹ çš„æ–¹æ³•å’Œæ€åº¦",
            "äººé™…äº¤å¾€çš„æ™ºæ…§",
            "é¢å¯¹å›°éš¾çš„æ€åº¦"
        ]
        
        selected_question = st.selectbox(
            "é€‰æ‹©ç¤ºä¾‹é—®é¢˜æˆ–è¾“å…¥è‡ªå®šä¹‰é—®é¢˜ï¼š",
            [""] + example_questions
        )
        
        # é—®é¢˜è¾“å…¥
        query = st.text_input(
            "æ‚¨çš„é—®é¢˜",
            value=selected_question if selected_question else "",
            placeholder="è¯·è¾“å…¥æ‚¨æƒ³äº†è§£çš„å¤æ–‡ç›¸å…³é—®é¢˜..."
        )
        
        # æœç´¢æŒ‰é’®
        col_btn1, col_btn2 = st.columns(2)
        with col_btn1:
            search_btn = st.button("ğŸ” æ™ºèƒ½æœç´¢", type="primary")
        with col_btn2:
            clear_btn = st.button("ğŸ—‘ï¸ æ¸…ç©ºç»“æœ")
        
        if clear_btn:
            st.session_state.last_results = None
            st.rerun()
        
        # å¤„ç†æœç´¢
        if search_btn and query:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æŸ¥è¯¢ä¼˜åŒ–
            use_optimization = st.session_state.get('query_optimization', False)
            
            if use_optimization and api_key and api_key.strip():
                with st.spinner("æ­£åœ¨ä¼˜åŒ–æŸ¥è¯¢..."):
                    optimized_query, optimization_status = st.session_state.rag_system.optimize_query(query, api_key)
                    
                # æ˜¾ç¤ºä¼˜åŒ–ç»“æœ
                if optimized_query != query:
                    st.info(f"ğŸ” **æŸ¥è¯¢ä¼˜åŒ–**: {optimization_status}")
                    st.write(f"**åŸå§‹é—®é¢˜**: {query}")
                    st.write(f"**ä¼˜åŒ–æŸ¥è¯¢**: {optimized_query}")
                    actual_query = optimized_query
                else:
                    actual_query = query
                    if optimization_status != "æœªä½¿ç”¨APIä¼˜åŒ–":
                        st.warning(f"æŸ¥è¯¢ä¼˜åŒ–: {optimization_status}")
            else:
                actual_query = query
            
            with st.spinner("æ­£åœ¨æœç´¢ç›¸å…³å¤æ–‡..."):
                results = st.session_state.rag_system.search(
                    actual_query, top_k, search_mode
                )
                st.session_state.last_results = (query, results, actual_query if use_optimization else None)
        
        # åœ¨æœç´¢ç»“æœæ˜¾ç¤ºä¸­æ·»åŠ é‡æ’åºåˆ†æ•°
        if hasattr(st.session_state, 'last_results') and st.session_state.last_results:
            # display_query, results = st.session_state.last_results

            if len(st.session_state.last_results) == 3:
                original_query, results, optimized_query = st.session_state.last_results
                display_query = optimized_query if optimized_query else original_query
            else:
                # å‘åå…¼å®¹
                display_query, results = st.session_state.last_results[:2]
            
            if results:
                st.subheader(f"ğŸ“– ã€Œ{display_query}ã€ç›¸å…³å¤æ–‡")
                
                for i, result in enumerate(results):
                    meta = result['metadata']
                                    
                    # æ„å»ºæ ‡é¢˜ï¼ŒåŒ…å«åˆ†æ•°
                    title_parts = [f"ğŸ“œ ã€Š{meta['book']}Â·{meta['chapter']}ã€‹"]

                    if 'rerank_score' in result and result['rerank_score'] > 0:
                        title_parts.append(f"(é‡æ’åº: {result['rerank_score']:.3f})")
                    elif 'combined_score' in result and result['combined_score'] > 0:
                        title_parts.append(f"(ç»¼åˆ: {result['combined_score']:.3f})")
                    elif 'vector_score' in result and result['vector_score'] > 0:
                        title_parts.append(f"(ç›¸ä¼¼åº¦: {result['vector_score']:.3f})")
                    elif 'bm25_score' in result and result['bm25_score'] > 0:
                        title_parts.append(f"(åŒ¹é…åº¦: {result['bm25_score']:.3f})")
                    else:
                        title_parts.append("(ç›¸å…³)")
                    
                    with st.expander(" ".join(title_parts)):
                        st.write(f"**åŸæ–‡å†…å®¹**ï¼š{result['content']}")
                        st.write(f"**è¯é¢˜åˆ†ç±»**ï¼š{meta['topic']}")
                        
                        # æ˜¾ç¤ºå„ç§åˆ†æ•°
                        if search_mode == 'hybrid' or 'rerank_score' in result:
                            score_cols = st.columns(4)
                            with score_cols[0]:
                                if 'bm25_score' in result:
                                    st.metric("BM25", f"{result['bm25_score']:.3f}")
                            with score_cols[1]:
                                if 'vector_score' in result:
                                    st.metric("å‘é‡", f"{result['vector_score']:.3f}")
                            with score_cols[2]:
                                if 'combined_score' in result:
                                    st.metric("èåˆ", f"{result['combined_score']:.3f}")
                            with score_cols[3]:
                                if 'rerank_score' in result:
                                    st.metric("é‡æ’åº", f"{result['rerank_score']:.3f}")
                        
                        # ä¸Šä¸‹æ–‡æ˜¾ç¤º
                        if 'context' in meta and meta['context'] != result['content']:
                            st.write("**ğŸ“„ ä¸Šä¸‹æ–‡ï¼š**")
                            st.text_area("", meta['context'], height=100, disabled=True, key=f"context_{i}")

                # ç”Ÿæˆæ™ºèƒ½å›ç­”
                st.subheader("ğŸ¤– æ™ºèƒ½è§£ç­”")
                with st.spinner("æ­£åœ¨ç”Ÿæˆæ™ºèƒ½å›ç­”..."):
                    answer = st.session_state.rag_system.generate_answer(
                        original_query, results, api_key
                    )
                    st.markdown(answer)
    with col2:
        st.header("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
        
        # æŠ€æœ¯ç‰¹ç‚¹
        st.info("""
        **ğŸ”§ æ ¸å¿ƒæŠ€æœ¯**
        - âœ… æ™ºèƒ½è¯­ä¹‰åˆ†å—
        - âœ… æ··åˆæ£€ç´¢ (BM25+å‘é‡)
        - âœ… å¤šæºæ•°æ®èåˆ
        - âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥
        - ğŸ”‘ AIæ™ºèƒ½é—®ç­”
        """)
        
        # æ”¯æŒçš„å¤æ–‡ç±»å‹
        st.subheader("ğŸ“š æ”¯æŒçš„å¤æ–‡")
        st.markdown("""
        - ğŸ“– **ç»å…¸**: è®ºè¯­ã€å­Ÿå­ã€å¤§å­¦ã€ä¸­åº¸
        - ğŸ“œ **å²ä¹¦**: å²è®°ã€æ±‰ä¹¦ç­‰å†å²æ–‡çŒ®
        - ğŸ­ **æ–‡å­¦**: è¯—ç»ã€æ¥šè¾ç­‰æ–‡å­¦ä½œå“
        - âš–ï¸ **æ³•å®¶**: éŸ©éå­ã€å•†å›ä¹¦ç­‰
        - ğŸ›ï¸ **å…¶ä»–**: å„ç±»å¤ä»£å…¸ç±
        
        *ç³»ç»Ÿé‡‡ç”¨é€šç”¨åŒ–è®¾è®¡ï¼Œå¯é€‚é…å„ç§å¤æ–‡æ ¼å¼*
        """)
        
        # ä½¿ç”¨è¯´æ˜
        st.subheader("ğŸ’¡ ä½¿ç”¨æŒ‡å—")
        st.markdown("""
        **ğŸ“ æ•°æ®å‡†å¤‡**ï¼š
        1. æŒ‰ `ä¹¦å/ç¯‡ç« /text.txt` ç»„ç»‡æ–‡ä»¶
        2. ç¡®ä¿æ–‡æœ¬ç¼–ç ä¸º UTF-8
        
        **ğŸ” æ£€ç´¢æ¨¡å¼**ï¼š
        - **æ··åˆæ£€ç´¢**: ç»¼åˆå…³é”®è¯+è¯­ä¹‰
        - **å‘é‡æ£€ç´¢**: çº¯è¯­ä¹‰ç›¸ä¼¼åº¦
        - **å…³é”®è¯æ£€ç´¢**: ä¼ ç»ŸBM25ç®—æ³•
        
        **ğŸ¯ æé—®æŠ€å·§**ï¼š
        - ä½¿ç”¨å¤æ–‡ä¸­çš„å…³é”®æ¦‚å¿µ
        - å¯ä»¥è¯¢é—®æ€æƒ³ã€äººç‰©ã€äº‹ä»¶
        - æ”¯æŒç°ä»£è¯­è¨€è¡¨è¾¾
        """)

if __name__ == "__main__":
    # è‡ªå®šä¹‰æ ·å¼
    st.markdown("""
    <style>
    .stApp {
        background-color: #fafafa;
    }
    .stButton>button {
        border-radius: 25px;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }
    .stExpander {
        border-radius: 15px;
        border: 1px solid #e0e0e0;
        margin-bottom: 10px;
    }
    .stSelectbox>div>div>div {
        border-radius: 10px;
    }
    .stTextInput>div>div>input {
        border-radius: 10px;
    }
    </style>
    """, unsafe_allow_html=True)
    
    main()